{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8835ad0-1649-4d0f-b01a-05186d60d505",
   "metadata": {},
   "source": [
    "# $\\text{Get PDF on Server}$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed4e605a-9aef-4dc9-8851-1d44f6c8fa28",
   "metadata": {},
   "source": [
    "cd ~/ECON847\n",
    "wget --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\" \\\n",
    "     -O 1998tarnicotinereport_0.pdf \\\n",
    "     \"https://www.ftc.gov/sites/default/files/documents/reports/2000-report-tar-nicotine-and-carbon-monoxide-covering-1998/1998tarnicotinereport_0.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79673e3-376d-4421-9a9f-ed69f7aa2a3b",
   "metadata": {},
   "source": [
    "# $\\text{PDF Cleaning}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a6436-0b02-4603-9afb-7c80669a809d",
   "metadata": {},
   "source": [
    "We extract cigrarettes product characteristics from [Federal Trade Commission Report of Cigarettes (1998)](https://www.ftc.gov/sites/default/files/documents/reports/2000-report-tar-nicotine-and-carbon-monoxide-covering-1998/1998tarnicotinereport_0.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cdafec5-bf16-4231-8852-66f79b852b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Characteristics/ftc_1998_tnco_clean.csv with 1333 rows\n"
     ]
    }
   ],
   "source": [
    "# save as extract_ftc_tnco_v2.py\n",
    "# run: python extract_ftc_tnco_v2.py /path/to/1998tarnicotinereport_0.pdf\n",
    "import sys, re, csv, os\n",
    "import pdfplumber\n",
    "\n",
    "PDF = \"Characteristics/1998tarnicotinereport.pdf\"\n",
    "OUT = \"Characteristics/ftc_1998_tnco_clean.csv\"\n",
    "PAGE_START, PAGE_END = 10, 45  # adjust if needed\n",
    "\n",
    "HDR_RE    = re.compile(r'(?i)\\bbrand\\s+name\\b.*\\bdescription\\b.*\\btar\\b.*\\bnic\\b.*\\bco\\b')\n",
    "NUMTOK_RE = re.compile(r'^(?:<)?\\d+(?:\\.\\d+)?$|^NA$', re.I)\n",
    "\n",
    "# Known tokens that indicate we've reached characteristics (not brand words)\n",
    "SIZE_TOKS   = {\"100\",\"100s\",\"100's\",\"KING\",\"KINGS\",\"REG\",\"REGULAR\"}\n",
    "PACK_TOKS   = {\"HP\",\"SP\"}  # hard/soft pack abbreviations used in the report\n",
    "FILTER_TOKS = {\"F\",\"NF\"}\n",
    "STRENGTH_TOKS = {\"FF\",\"FULL\",\"FLAVOR\",\"LIGHT\",\"LT\",\"ULTRA\",\"ULTRA-LT\",\"ULTIMA\",\"MILD\"}\n",
    "OTHER_CHAR_TOKS = {\"MENTHOL\",\"DLX\",\"DELUXE\",\"SPECIAL\",\"SUP\",\"SLIM\",\"SUPERSLIM\",\"SUPER\",\"BOX\",\"BOXES\",\"GENERIC\"}\n",
    "\n",
    "CHAR_STARTERS = SIZE_TOKS | PACK_TOKS | FILTER_TOKS | STRENGTH_TOKS | OTHER_CHAR_TOKS\n",
    "\n",
    "# Value/generic-style brands to flag (you can extend this)\n",
    "VALUE_BRANDS = {\n",
    "    \"BEST BUY\", \"BARGAIN BUY\", \"ALL AMERICAN VALUE\", \"ALL-AMERICAN VALUE\",\n",
    "    \"AMERICAN VALUE\"\n",
    "}\n",
    "\n",
    "def find_header_cuts(page):\n",
    "    words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "    header_words = [w for w in words if w[\"text\"].lower() in {\"brand\",\"name\",\"description\",\"tar\",\"nic\",\"co\"}]\n",
    "    if not header_words:\n",
    "        return None, None, None\n",
    "\n",
    "    # group header words by line\n",
    "    y_key = round(sorted(header_words, key=lambda w: w[\"top\"])[0][\"top\"]/3.0)\n",
    "    ws = [w for w in header_words if round(w[\"top\"]/3.0)==y_key]\n",
    "\n",
    "    # x starts\n",
    "    def x(label):\n",
    "        xs = [w[\"x0\"] for w in ws if w[\"text\"].lower()==label]\n",
    "        return min(xs) if xs else None\n",
    "\n",
    "    x_brand = x(\"brand\"); x_name = x(\"name\"); x_desc = x(\"description\")\n",
    "    x_tar   = x(\"tar\");   x_nic  = x(\"nic\");  x_co   = x(\"co\")\n",
    "    if None in (x_brand,x_name,x_desc,x_tar,x_nic,x_co):\n",
    "        return None, None, None\n",
    "\n",
    "    x_brand_start = min(x_brand, x_name)\n",
    "    x_brand_end   = (x_desc + x_brand_start)/2.0\n",
    "    x_desc_end    = (x_tar + x_desc)/2.0\n",
    "    x_tar_end     = (x_nic + x_tar)/2.0\n",
    "    x_nic_end     = (x_co + x_nic)/2.0\n",
    "\n",
    "    header_top    = min(w[\"top\"] for w in ws)\n",
    "    header_bottom = max(w[\"bottom\"] for w in ws)\n",
    "    return (x_brand_start, x_brand_end, x_desc_end, x_tar_end, x_nic_end), header_top, header_bottom\n",
    "\n",
    "def bucketize(words, cuts, header_bottom):\n",
    "    x_brand_start, x_brand_end, x_desc_end, x_tar_end, x_nic_end = cuts\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        y_mid = (w[\"top\"] + w[\"bottom\"]) / 2.0\n",
    "        if y_mid <= header_bottom + 1.0:\n",
    "            continue\n",
    "        key = round(y_mid / 2.0, 1)\n",
    "        lines.setdefault(key, []).append(w)\n",
    "\n",
    "    rows = []\n",
    "    for key in sorted(lines.keys()):\n",
    "        ws = sorted(lines[key], key=lambda z: z[\"x0\"])\n",
    "        cols = {\"brand\": [], \"desc\": [], \"tar\": [], \"nic\": [], \"co\": []}\n",
    "        for w in ws:\n",
    "            xm = (w[\"x0\"] + w[\"x1\"]) / 2.0\n",
    "            txt = w[\"text\"]\n",
    "            if xm < x_brand_end:\n",
    "                cols[\"brand\"].append(txt)\n",
    "            elif xm < x_desc_end:\n",
    "                cols[\"desc\"].append(txt)\n",
    "            elif xm < x_tar_end:\n",
    "                cols[\"tar\"].append(txt)\n",
    "            elif xm < x_nic_end:\n",
    "                cols[\"nic\"].append(txt)\n",
    "            else:\n",
    "                cols[\"co\"].append(txt)\n",
    "\n",
    "        brand = \" \".join(cols[\"brand\"]).strip()\n",
    "        desc  = \" \".join(cols[\"desc\"]).strip()\n",
    "        tar   = \" \".join(cols[\"tar\"]).strip()\n",
    "        nic   = \" \".join(cols[\"nic\"]).strip()\n",
    "        co    = \" \".join(cols[\"co\"]).strip()\n",
    "\n",
    "        # If TNCO aren't clean, try right-anchored salvage\n",
    "        if not (NUMTOK_RE.match(tar or \"\") and NUMTOK_RE.match(nic or \"\") and NUMTOK_RE.match(co or \"\")):\n",
    "            toks = (brand + \"  \" + desc + \"  \" + tar + \" \" + nic + \" \" + co).split()\n",
    "            idxs = [i for i,t in enumerate(toks) if NUMTOK_RE.match(t)]\n",
    "            if len(idxs) >= 3:\n",
    "                i3,i2,i1 = idxs[-1],idxs[-2],idxs[-3]\n",
    "                co,nic,tar = toks[i3], toks[i2], toks[i1]\n",
    "                left = \" \".join(toks[:i1])\n",
    "                m = re.search(r'\\s{2,}', left)\n",
    "                if m:\n",
    "                    brand = left[:m.start()].strip()\n",
    "                    desc  = left[m.end():].strip()\n",
    "                else:\n",
    "                    parts = left.split()\n",
    "                    brand = parts[0] if parts else \"\"\n",
    "                    desc  = \" \".join(parts[1:]) if len(parts)>1 else \"\"\n",
    "\n",
    "        if NUMTOK_RE.match(tar or \"\") and NUMTOK_RE.match(nic or \"\") and NUMTOK_RE.match(co or \"\"):\n",
    "            rows.append([brand, desc, tar, nic, co])\n",
    "    return rows\n",
    "\n",
    "def leading_brand_fix(brand, desc):\n",
    "    \"\"\"\n",
    "    Move leading brand words from desc into brand until a characteristic token appears.\n",
    "    Handles cases like:\n",
    "      brand='Benson &' desc='& Hedges King F HP'  -> 'Benson & Hedges' + 'King F HP'\n",
    "      brand='Best'     desc='Buy* 100 F HP'       -> 'Best Buy*'      + '100 F HP'\n",
    "      brand='All'      desc='American Value 100'  -> 'All American Value' + '100'\n",
    "    \"\"\"\n",
    "    if not desc:\n",
    "        return brand, desc, 0\n",
    "    tokens = desc.split()\n",
    "    moved = []\n",
    "    star = 1 if (\"*\" in brand) else 0\n",
    "\n",
    "    # move '& Something' or other capitalized words until a characteristic token\n",
    "    while tokens:\n",
    "        t = tokens[0]\n",
    "        upper = re.sub(r'[^A-Za-z0-9&-]', '', t).upper()\n",
    "        # stop if token is obviously a characteristic or number (e.g., 100)\n",
    "        if upper in CHAR_STARTERS or NUMTOK_RE.match(t):\n",
    "            break\n",
    "        # likely part of brand -> move it\n",
    "        moved.append(tokens.pop(0))\n",
    "\n",
    "    if moved:\n",
    "        brand = (brand + \" \" + \" \".join(moved)).strip()\n",
    "\n",
    "    # if desc started with \"& Hedges\" and brand was \"Benson &\", above will move \"&\" and \"Hedges\"\n",
    "    # remove any '*' that came with moved piece\n",
    "    if \"*\" in brand:\n",
    "        star = 1\n",
    "        brand = brand.replace(\"*\",\"\").strip()\n",
    "\n",
    "    desc = \" \".join(tokens).strip()\n",
    "    return brand, desc, star\n",
    "\n",
    "def parse_characteristics(desc):\n",
    "    d = (desc or \"\").strip()\n",
    "\n",
    "    # Size with no missing: prefer 100, then King; otherwise Reg\n",
    "    if re.search(r'(?<!\\d)100(?!\\d)', d):\n",
    "        size = \"100\"\n",
    "    elif re.search(r'(?i)\\bKing(s)?\\b', d):\n",
    "        size = \"King\"\n",
    "    else:\n",
    "        size = \"Reg\"\n",
    "\n",
    "    # Dummies\n",
    "    F  = 1 if re.search(r'(?i)(^|[^\\w])F($|[^\\w])|Full\\s*Filter', d) else 0\n",
    "    NF = 1 if re.search(r'(?i)(^|[^\\w])NF($|[^\\w])|Non[-\\s]*Filter', d) else 0\n",
    "    HP = 1 if re.search(r'(?i)\\bHP\\b|Hard\\s*Pack', d) else 0\n",
    "    SP = 1 if re.search(r'(?i)\\bSP\\b|Soft\\s*Pack', d) else 0\n",
    "\n",
    "    ULTRA = 1 if re.search(r'(?i)Ultra(?:-|\\s*)Lt|Ultra Light|Ultima', d) else 0\n",
    "    LT    = 1 if (not ULTRA) and re.search(r'(?i)\\bLt\\b|Light', d) else 0\n",
    "    FF    = 1 if re.search(r'(?i)\\bFF\\b|Full\\s*Flavor', d) else 0\n",
    "\n",
    "    MENTHOL = 1 if re.search(r'(?i)Menthol', d) else 0\n",
    "    GENERIC = 1 if re.search(r'(?i)\\bGeneric\\b', d) else 0\n",
    "    DLX     = 1 if re.search(r'(?i)\\bDLX\\b|Deluxe', d) else 0\n",
    "    SPECIAL = 1 if re.search(r'(?i)\\bSpecial\\b', d) else 0\n",
    "    # \"Sup Slim\" / \"Super Slim\" / \"Superslim\" / \"Slim\"\n",
    "    SUPSLIM = 1 if re.search(r'(?i)Sup(?:er)?\\s*Slim|Superslim|Super[-\\s]*Slim', d) else 0\n",
    "    SLIM    = 1 if (not SUPSLIM) and re.search(r'(?i)\\bSlim(s)?\\b', d) else 0\n",
    "\n",
    "    return size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM\n",
    "\n",
    "def split_op_val(s):\n",
    "    s = (s or \"\").strip()\n",
    "    op = \"<\" if s.startswith(\"<\") else \"\"\n",
    "    val = s[1:] if op == \"<\" else s\n",
    "    return op, val\n",
    "\n",
    "all_rows = []\n",
    "with pdfplumber.open(PDF) as pdf:\n",
    "    for pno in range(PAGE_START, PAGE_END + 1):\n",
    "        page = pdf.pages[pno-1]\n",
    "        text = page.extract_text() or \"\"\n",
    "        if not text or not HDR_RE.search(text):\n",
    "            continue\n",
    "\n",
    "        cuts, header_top, header_bottom = find_header_cuts(page)\n",
    "        if not cuts:\n",
    "            continue\n",
    "\n",
    "        words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "        page_rows = bucketize(words, cuts, header_bottom)\n",
    "        all_rows.extend(page_rows)\n",
    "\n",
    "cleaned = []\n",
    "for brand, desc, Tar, Nic, CO in all_rows:\n",
    "    if not (brand or desc):\n",
    "        continue\n",
    "    # strip spaces\n",
    "    brand = (brand or \"\").strip()\n",
    "    desc  = (desc  or \"\").strip()\n",
    "\n",
    "    # move brand fragments from desc -> brand (Benson & | & Hedges, Best | Buy*, etc.)\n",
    "    brand, desc, star_from_move = leading_brand_fix(brand, desc)\n",
    "\n",
    "    # manufacturer-tested star: from brand OR anything we moved\n",
    "    manufacturer_tested = 1 if (\"*\" in brand or star_from_move) else 0\n",
    "    brand = brand.replace(\"*\",\"\").strip()\n",
    "\n",
    "    # drop any header echoes\n",
    "    if re.match(r'(?i)^brand\\s*name$', brand) or re.match(r'(?i)^description$', desc):\n",
    "        continue\n",
    "\n",
    "    # parse characteristics (ensure Size has no missing: 100/King/Reg)\n",
    "    size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM = parse_characteristics(desc)\n",
    "\n",
    "    # TNCO split\n",
    "    Tar_op, Tar_val = split_op_val(Tar)\n",
    "    Nic_op, Nic_val = split_op_val(Nic)\n",
    "    CO_op,  CO_val  = split_op_val(CO)\n",
    "\n",
    "    # Value-brand flag\n",
    "    value_brand = 1 if brand.upper() in VALUE_BRANDS else 0\n",
    "\n",
    "    cleaned.append([\n",
    "        brand, desc, size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM,\n",
    "        Tar, Nic, CO, Tar_op, Tar_val, Nic_op, Nic_val, CO_op, CO_val, manufacturer_tested, value_brand\n",
    "    ])\n",
    "\n",
    "# write CSV\n",
    "os.makedirs(os.path.dirname(os.path.abspath(OUT)), exist_ok=True)\n",
    "with open(OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\n",
    "        \"Brand Name\",\"Description\",\"Size\",\"F\",\"NF\",\"HP\",\"SP\",\"Ultra_Lt\",\"Lt\",\"FF\",\"Menthol\",\n",
    "        \"Generic\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\n",
    "        \"Tar\",\"Nic\",\"CO\",\"Tar_op\",\"Tar_value\",\"Nic_op\",\"Nic_value\",\"CO_op\",\"CO_value\",\n",
    "        \"manufacturer_tested\",\"value_brand\"\n",
    "    ])\n",
    "    w.writerows(cleaned)\n",
    "\n",
    "print(f\"Wrote {OUT} with {len(cleaned)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e199a-590b-42f9-9642-73f195f65efc",
   "metadata": {},
   "source": [
    "# $\\text{UPC Cleaning}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fd04ab1-7cef-4997-80b0-ba604084361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading UPC  : F:\\Codes\\Jupyter\\ECON847\\PS2\\Dominick\\upccig.csv\n",
      "Reading FTC  : F:\\Codes\\Jupyter\\ECON847\\PS2\\Characteristics\\ftc_1998_tnco_clean.csv\n",
      "Saving to    : F:\\Codes\\Jupyter\\ECON847\\PS2\\Characteristics\n",
      "Wrote: F:\\Codes\\Jupyter\\ECON847\\PS2\\Characteristics\\upccig_clean.csv\n",
      "Wrote: F:\\Codes\\Jupyter\\ECON847\\PS2\\Characteristics\\upccig_with_tnco.csv\n",
      "Rows: 942 | matched to FTC at any level: 160\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv, re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== 1) Set your PS2 root once =====\n",
    "ROOT = Path(r\"F:\\Codes\\Jupyter\\ECON847\\PS2\")  # <- change if needed\n",
    "\n",
    "UPC_PATH = ROOT / \"Dominick\" / \"upccig.csv\"\n",
    "FTC_PATH = ROOT / \"Characteristics\" / \"ftc_1998_tnco_clean.csv\"\n",
    "OUT_DIR  = FTC_PATH.parent\n",
    "\n",
    "print(\"Reading UPC  :\", UPC_PATH)\n",
    "print(\"Reading FTC  :\", FTC_PATH)\n",
    "print(\"Saving to    :\", OUT_DIR)\n",
    "\n",
    "# ===== 2) Helpers =====\n",
    "def norm(s: str) -> str:\n",
    "    s = (s or \"\").upper()\n",
    "    s = re.sub(r\"[^\\w&]+\", \" \", s)        # keep letters/digits/_/&\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def parse_size(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    if re.search(r\"(?<!\\d)100(?!\\d)|\\b100S?\\b\", t): return \"100\"\n",
    "    if re.search(r\"\\bKING(S)?\\b\", t):               return \"King\"\n",
    "    return \"Reg\"  # no missing\n",
    "\n",
    "def parse_pack(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    if re.search(r\"\\b(HP|HARD\\s*PACK|BOX|BOXES)\\b\", t): return \"HP\"\n",
    "    if re.search(r\"\\b(SP|SOFT\\s*PACK)\\b\", t):           return \"SP\"\n",
    "    return \"UNK\"  # fall back to brand+size\n",
    "\n",
    "def parse_char_dummies(text: str):\n",
    "    t = norm(text)\n",
    "    menthol = 1 if \"MENTHOL\" in t else 0\n",
    "    dlx     = 1 if re.search(r\"\\b(DLX|DELUXE)\\b\", t) else 0\n",
    "    special = 1 if re.search(r\"\\bSPECIAL\\b\", t) else 0\n",
    "    supslim = 1 if re.search(r\"\\b(SUPER\\s*SLIM|SUPSLIM|SUPERSLIM)\\b\", t) else 0\n",
    "    slim    = 1 if (not supslim) and re.search(r\"\\bSLIM(S)?\\b\", t) else 0\n",
    "    generic = 1 if re.search(r\"\\bGENERIC\\b\", t) else 0\n",
    "    carton  = 1 if re.search(r\"\\bCARTON(S)?\\b\", t) else 0\n",
    "    single  = 1 if re.search(r\"\\bSINGLE\\b\", t) else 0\n",
    "    pack_kw = 1 if re.search(r\"\\bPACK\\b|\\bPK\\b\", t) else 0\n",
    "    return menthol, dlx, special, supslim, slim, generic, carton, single, pack_kw\n",
    "\n",
    "def safe_float(x):\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s: return None\n",
    "    try: return float(s)\n",
    "    except:\n",
    "        try: return float(s.replace(\",\", \"\"))\n",
    "        except: return None\n",
    "\n",
    "# ---- brand lexicon from FTC (longest match, read with errors='replace') ----\n",
    "def build_brand_patterns(ftc_csv_path: Path):\n",
    "    brands = set()\n",
    "    with ftc_csv_path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            b = (row.get(\"Brand Name\") or \"\").strip()\n",
    "            if b: brands.add(b)\n",
    "    # a couple common value-brand aliases\n",
    "    brands |= {\"ALL AMERICAN VALUE\", \"AMERICAN VALUE\", \"ALL-AMERICAN VALUE\"}\n",
    "\n",
    "    pats = []\n",
    "    for b in brands:\n",
    "        tokens = [re.escape(x) for x in norm(b).split()]\n",
    "        if tokens:\n",
    "            pats.append((b, re.compile(r\"\\b\" + r\"\\s*\".join(tokens) + r\"\\b\")))\n",
    "    pats.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    return pats\n",
    "\n",
    "def find_brand(descrip: str, brand_patterns):\n",
    "    T = norm(descrip)\n",
    "    for brand, pat in brand_patterns:\n",
    "        if pat.search(T):\n",
    "            return brand\n",
    "    return \"\"\n",
    "\n",
    "# ---- FTC aggregates by (brand,size,pack) → (brand,size) → (brand) ----\n",
    "def load_ftc_aggregates(ftc_csv_path: Path):\n",
    "    b_sp = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "    b_s  = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "    b    = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "\n",
    "    with ftc_csv_path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        has_val = {\"Tar_value\",\"Nic_value\",\"CO_value\"}.issubset(r.fieldnames or [])\n",
    "        for row in r:\n",
    "            brand = (row.get(\"Brand Name\") or \"\").strip()\n",
    "            if not brand: \n",
    "                continue\n",
    "            size  = (row.get(\"Size\") or \"Reg\").strip()\n",
    "\n",
    "            hp = str(row.get(\"HP\") or \"\").strip()\n",
    "            sp = str(row.get(\"SP\") or \"\").strip()\n",
    "            pack = \"HP\" if hp in (\"1\",\"True\",\"TRUE\",\"true\") else (\"SP\" if sp in (\"1\",\"True\",\"TRUE\",\"true\") else \"UNK\")\n",
    "\n",
    "            if has_val:\n",
    "                tar = safe_float(row.get(\"Tar_value\"))\n",
    "                nic = safe_float(row.get(\"Nic_value\"))\n",
    "                co  = safe_float(row.get(\"CO_value\"))\n",
    "            else:\n",
    "                def num_from_raw(v):\n",
    "                    s = (v or \"\").strip()\n",
    "                    s = s[1:] if s.startswith(\"<\") else s\n",
    "                    return safe_float(s)\n",
    "                tar = num_from_raw(row.get(\"Tar\"))\n",
    "                nic = num_from_raw(row.get(\"Nic\"))\n",
    "                co  = num_from_raw(row.get(\"CO\"))\n",
    "\n",
    "            for name, val in ((\"tar\",tar), (\"nic\",nic), (\"co\",co)):\n",
    "                if val is None: \n",
    "                    continue\n",
    "                b_sp[(brand,size,pack)][name].append(val)\n",
    "                b_s[(brand,size)][name].append(val)\n",
    "                b[(brand,)][name].append(val)\n",
    "\n",
    "    def reduce_means(B):\n",
    "        out = {}\n",
    "        for k, vv in B.items():\n",
    "            def m(x): return sum(x)/len(x) if x else None\n",
    "            out[k] = (m(vv[\"tar\"]), m(vv[\"nic\"]), m(vv[\"co\"]))\n",
    "        return out\n",
    "\n",
    "    return reduce_means(b_sp), reduce_means(b_s), reduce_means(b)\n",
    "\n",
    "# ===== 3) Clean & merge (reads UPC with errors='replace') =====\n",
    "def clean_and_merge(UPC_PATH: Path, FTC_PATH: Path, OUT_DIR: Path):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_clean = OUT_DIR / \"upccig_clean.csv\"\n",
    "    out_merge = OUT_DIR / \"upccig_with_tnco.csv\"\n",
    "\n",
    "    brand_patterns = build_brand_patterns(FTC_PATH)\n",
    "    by_bsp, by_bs, by_b = load_ftc_aggregates(FTC_PATH)\n",
    "\n",
    "    with UPC_PATH.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as fin, \\\n",
    "         out_clean.open(\"w\", newline=\"\", encoding=\"utf-8\") as fclean, \\\n",
    "         out_merge.open(\"w\", newline=\"\", encoding=\"utf-8\") as fmerge:\n",
    "\n",
    "        rin = csv.DictReader(fin)\n",
    "        cols_in = rin.fieldnames or []\n",
    "        clean_cols = cols_in + [\"brand\",\"size\",\"pack\",\"Menthol\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\"Generic\",\"Carton\",\"Single\",\"Pack_kw\"]\n",
    "        rout_clean = csv.DictWriter(fclean, fieldnames=clean_cols)\n",
    "        rout_clean.writeheader()\n",
    "\n",
    "        merge_cols = clean_cols + [\"tnco_key_level\",\"Tar_mean\",\"Nic_mean\",\"CO_mean\"]\n",
    "        rout_merge = csv.DictWriter(fmerge, fieldnames=merge_cols)\n",
    "        rout_merge.writeheader()\n",
    "\n",
    "        n, matched = 0, 0\n",
    "        for row in rin:\n",
    "            n += 1\n",
    "            descr = (row.get(\"DESCRIP\") or row.get(\"DESCR\") or \"\").strip()\n",
    "\n",
    "            brand = find_brand(descr, brand_patterns) or \"\"   # NA if not found\n",
    "            size  = parse_size(descr)\n",
    "            pack  = parse_pack(descr)                         # HP / SP / UNK\n",
    "\n",
    "            Menthol, Dlx, Special, SupSlim, Slim, Generic, Carton, Single, Pack_kw = parse_char_dummies(descr)\n",
    "\n",
    "            clean_row = dict(row)\n",
    "            clean_row.update({\n",
    "                \"brand\": brand if brand else \"NA\",\n",
    "                \"size\": size,\n",
    "                \"pack\": pack,\n",
    "                \"Menthol\": Menthol, \"Dlx\": Dlx, \"Special\": Special,\n",
    "                \"SupSlim\": SupSlim, \"Slim\": Slim, \"Generic\": Generic,\n",
    "                \"Carton\": Carton, \"Single\": Single, \"Pack_kw\": Pack_kw\n",
    "            })\n",
    "            rout_clean.writerow(clean_row)\n",
    "\n",
    "            # Merge priority: (brand,size,pack) → (brand,size) → (brand)\n",
    "            tnco_key_level = \"none\"\n",
    "            Tar_mean = Nic_mean = CO_mean = \"\"\n",
    "\n",
    "            if brand:\n",
    "                if pack in (\"HP\",\"SP\") and (brand,size,pack) in by_bsp:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_bsp[(brand,size,pack)]; tnco_key_level = \"brand+size+pack\"\n",
    "                elif (brand,size) in by_bs:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_bs[(brand,size)]; tnco_key_level = \"brand+size\"\n",
    "                elif (brand,) in by_b:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_b[(brand,)]; tnco_key_level = \"brand\"\n",
    "\n",
    "            matched += (tnco_key_level != \"none\")\n",
    "\n",
    "            merge_row = dict(clean_row)\n",
    "            merge_row.update({\n",
    "                \"tnco_key_level\": tnco_key_level,\n",
    "                \"Tar_mean\": f\"{Tar_mean:.3f}\" if isinstance(Tar_mean,float) else \"\",\n",
    "                \"Nic_mean\": f\"{Nic_mean:.3f}\" if isinstance(Nic_mean,float) else \"\",\n",
    "                \"CO_mean\":  f\"{CO_mean:.3f}\"  if isinstance(CO_mean, float) else \"\",\n",
    "            })\n",
    "            rout_merge.writerow(merge_row)\n",
    "\n",
    "    print(f\"Wrote: {out_clean}\")\n",
    "    print(f\"Wrote: {out_merge}\")\n",
    "    print(f\"Rows: {n} | matched to FTC at any level: {matched}\")\n",
    "\n",
    "# run\n",
    "clean_and_merge(UPC_PATH, FTC_PATH, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ddef451-1fea-4117-b4af-b2b70bde32e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: F:\\Codes\\Jupyter\\ECON847\\PS2\\Characteristics\\upccig_with_tnco.csv\n",
      "Rows total: 942\n",
      "Unique DESCRIP: 95\n",
      "Unique UPC: 942\n",
      "Rows with clear brand: 160  (16.99%)\n",
      "DESCRIP coverage (unique descriptions with a clear brand): 27.37%\n",
      "UPC coverage (unique UPCs with a clear brand): 16.99%\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv, re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---- set your PS2 root folder once ----\n",
    "ROOT = Path(r\"F:\\Codes\\Jupyter\\ECON847\\PS2\")   # <-- adjust if needed\n",
    "INPATH = ROOT / \"Characteristics\" / \"upccig_with_tnco.csv\"\n",
    "OUTDIR = INPATH.parent\n",
    "\n",
    "print(\"Reading:\", INPATH)\n",
    "\n",
    "def is_clear_brand(b):\n",
    "    b = (b or \"\").strip().upper()\n",
    "    return b not in {\"\", \"NA\", \"N/A\", \"UNK\", \"UNKNOWN\"}\n",
    "\n",
    "# find a UPC column name robustly\n",
    "def pick_upc_col(fieldnames):\n",
    "    if not fieldnames: return None\n",
    "    # prefer exact names, then any column containing 'UPC'\n",
    "    prefs = [\"UPC\", \"UPC12\", \"UPC_CODE\", \"UPCNUM\", \"UPC CODE\", \"UPCNUMBER\"]\n",
    "    for p in prefs:\n",
    "        if p in fieldnames: return p\n",
    "    for fn in fieldnames:\n",
    "        if re.search(r\"\\bUPC\\b\", fn, re.I): return fn\n",
    "    return None\n",
    "\n",
    "n_rows = 0\n",
    "uniq_descr = set()\n",
    "uniq_upc   = set()\n",
    "rows_with_brand = 0\n",
    "\n",
    "# coverage by description / by upc\n",
    "descr_has_brand = defaultdict(lambda: False)\n",
    "upc_has_brand   = defaultdict(lambda: False)\n",
    "\n",
    "with INPATH.open(\"r\", encoding=\"utf-8\", errors=\"replace\", newline=\"\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    fields = r.fieldnames or []\n",
    "    upc_col = pick_upc_col(fields)\n",
    "    if upc_col is None:\n",
    "        print(\"WARNING: Couldn't find a UPC column; I'll treat all UPCs as blank.\")\n",
    "\n",
    "    for row in r:\n",
    "        n_rows += 1\n",
    "        descr = (row.get(\"DESCRIP\") or row.get(\"DESCR\") or \"\").strip()\n",
    "        upc   = (row.get(upc_col) if upc_col else \"\").strip()\n",
    "        brand = row.get(\"brand\", \"\")\n",
    "\n",
    "        uniq_descr.add(descr)\n",
    "        uniq_upc.add(upc)\n",
    "\n",
    "        if is_clear_brand(brand):\n",
    "            rows_with_brand += 1\n",
    "            descr_has_brand[descr] = True\n",
    "            upc_has_brand[upc]     = True\n",
    "\n",
    "# compute ratios\n",
    "row_ratio     = rows_with_brand / n_rows if n_rows else 0.0\n",
    "descr_ratio   = (sum(descr_has_brand.values()) / len(uniq_descr)) if uniq_descr else 0.0\n",
    "upc_ratio     = (sum(upc_has_brand.values())   / len(uniq_upc))   if uniq_upc else 0.0\n",
    "\n",
    "print(f\"Rows total: {n_rows:,}\")\n",
    "print(f\"Unique DESCRIP: {len(uniq_descr):,}\")\n",
    "print(f\"Unique UPC: {len(uniq_upc):,}\")\n",
    "print(f\"Rows with clear brand: {rows_with_brand:,}  ({row_ratio:.2%})\")\n",
    "print(f\"DESCRIP coverage (unique descriptions with a clear brand): {descr_ratio:.2%}\")\n",
    "print(f\"UPC coverage (unique UPCs with a clear brand): {upc_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b9afd-dc5d-4025-84be-a6b87c604033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as extract_ftc_tnco_v2.py\n",
    "# run: python extract_ftc_tnco_v2.py /path/to/1998tarnicotinereport_0.pdf\n",
    "import sys, re, csv, os\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "PDF = \"1998tarnicotinereport_0.pdf\"\n",
    "OUT = \"ftc_1998_tnco_clean.csv\"\n",
    "PAGE_START, PAGE_END = 10, 45  # adjust if needed\n",
    "\n",
    "HDR_RE    = re.compile(r'(?i)\\bbrand\\s+name\\b.*\\bdescription\\b.*\\btar\\b.*\\bnic\\b.*\\bco\\b')\n",
    "NUMTOK_RE = re.compile(r'^(?:<)?\\d+(?:\\.\\d+)?$|^NA$', re.I)\n",
    "\n",
    "# Known tokens that indicate we've reached characteristics (not brand words)\n",
    "SIZE_TOKS   = {\"100\",\"100s\",\"100's\",\"KING\",\"KINGS\",\"REG\",\"REGULAR\"}\n",
    "PACK_TOKS   = {\"HP\",\"SP\"}  # hard/soft pack abbreviations used in the report\n",
    "FILTER_TOKS = {\"F\",\"NF\"}\n",
    "STRENGTH_TOKS = {\"FF\",\"FULL\",\"FLAVOR\",\"LIGHT\",\"LT\",\"ULTRA\",\"ULTRA-LT\",\"ULTIMA\",\"MILD\"}\n",
    "OTHER_CHAR_TOKS = {\"MENTHOL\",\"DLX\",\"DELUXE\",\"SPECIAL\",\"SUP\",\"SLIM\",\"SUPERSLIM\",\"SUPER\",\"BOX\",\"BOXES\",\"GENERIC\"}\n",
    "\n",
    "CHAR_STARTERS = SIZE_TOKS | PACK_TOKS | FILTER_TOKS | STRENGTH_TOKS | OTHER_CHAR_TOKS\n",
    "\n",
    "# Value/generic-style brands to flag (you can extend this)\n",
    "VALUE_BRANDS = {\n",
    "    \"BEST BUY\", \"BARGAIN BUY\", \"ALL AMERICAN VALUE\", \"ALL-AMERICAN VALUE\",\n",
    "    \"AMERICAN VALUE\"\n",
    "}\n",
    "\n",
    "def find_header_cuts(page):\n",
    "    words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "    header_words = [w for w in words if w[\"text\"].lower() in {\"brand\",\"name\",\"description\",\"tar\",\"nic\",\"co\"}]\n",
    "    if not header_words:\n",
    "        return None, None, None\n",
    "\n",
    "    # group header words by line\n",
    "    y_key = round(sorted(header_words, key=lambda w: w[\"top\"])[0][\"top\"]/3.0)\n",
    "    ws = [w for w in header_words if round(w[\"top\"]/3.0)==y_key]\n",
    "\n",
    "    # x starts\n",
    "    def x(label):\n",
    "        xs = [w[\"x0\"] for w in ws if w[\"text\"].lower()==label]\n",
    "        return min(xs) if xs else None\n",
    "\n",
    "    x_brand = x(\"brand\"); x_name = x(\"name\"); x_desc = x(\"description\")\n",
    "    x_tar   = x(\"tar\");   x_nic  = x(\"nic\");  x_co   = x(\"co\")\n",
    "    if None in (x_brand,x_name,x_desc,x_tar,x_nic,x_co):\n",
    "        return None, None, None\n",
    "\n",
    "    x_brand_start = min(x_brand, x_name)\n",
    "    x_brand_end   = (x_desc + x_brand_start)/2.0\n",
    "    x_desc_end    = (x_tar + x_desc)/2.0\n",
    "    x_tar_end     = (x_nic + x_tar)/2.0\n",
    "    x_nic_end     = (x_co + x_nic)/2.0\n",
    "\n",
    "    header_top    = min(w[\"top\"] for w in ws)\n",
    "    header_bottom = max(w[\"bottom\"] for w in ws)\n",
    "    return (x_brand_start, x_brand_end, x_desc_end, x_tar_end, x_nic_end), header_top, header_bottom\n",
    "\n",
    "def bucketize(words, cuts, header_bottom):\n",
    "    x_brand_start, x_brand_end, x_desc_end, x_tar_end, x_nic_end = cuts\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        y_mid = (w[\"top\"] + w[\"bottom\"]) / 2.0\n",
    "        if y_mid <= header_bottom + 1.0:\n",
    "            continue\n",
    "        key = round(y_mid / 2.0, 1)\n",
    "        lines.setdefault(key, []).append(w)\n",
    "\n",
    "    rows = []\n",
    "    for key in sorted(lines.keys()):\n",
    "        ws = sorted(lines[key], key=lambda z: z[\"x0\"])\n",
    "        cols = {\"brand\": [], \"desc\": [], \"tar\": [], \"nic\": [], \"co\": []}\n",
    "        for w in ws:\n",
    "            xm = (w[\"x0\"] + w[\"x1\"]) / 2.0\n",
    "            txt = w[\"text\"]\n",
    "            if xm < x_brand_end:\n",
    "                cols[\"brand\"].append(txt)\n",
    "            elif xm < x_desc_end:\n",
    "                cols[\"desc\"].append(txt)\n",
    "            elif xm < x_tar_end:\n",
    "                cols[\"tar\"].append(txt)\n",
    "            elif xm < x_nic_end:\n",
    "                cols[\"nic\"].append(txt)\n",
    "            else:\n",
    "                cols[\"co\"].append(txt)\n",
    "\n",
    "        brand = \" \".join(cols[\"brand\"]).strip()\n",
    "        desc  = \" \".join(cols[\"desc\"]).strip()\n",
    "        tar   = \" \".join(cols[\"tar\"]).strip()\n",
    "        nic   = \" \".join(cols[\"nic\"]).strip()\n",
    "        co    = \" \".join(cols[\"co\"]).strip()\n",
    "\n",
    "        # If TNCO aren't clean, try right-anchored salvage\n",
    "        if not (NUMTOK_RE.match(tar or \"\") and NUMTOK_RE.match(nic or \"\") and NUMTOK_RE.match(co or \"\")):\n",
    "            toks = (brand + \"  \" + desc + \"  \" + tar + \" \" + nic + \" \" + co).split()\n",
    "            idxs = [i for i,t in enumerate(toks) if NUMTOK_RE.match(t)]\n",
    "            if len(idxs) >= 3:\n",
    "                i3,i2,i1 = idxs[-1],idxs[-2],idxs[-3]\n",
    "                co,nic,tar = toks[i3], toks[i2], toks[i1]\n",
    "                left = \" \".join(toks[:i1])\n",
    "                m = re.search(r'\\s{2,}', left)\n",
    "                if m:\n",
    "                    brand = left[:m.start()].strip()\n",
    "                    desc  = left[m.end():].strip()\n",
    "                else:\n",
    "                    parts = left.split()\n",
    "                    brand = parts[0] if parts else \"\"\n",
    "                    desc  = \" \".join(parts[1:]) if len(parts)>1 else \"\"\n",
    "\n",
    "        if NUMTOK_RE.match(tar or \"\") and NUMTOK_RE.match(nic or \"\") and NUMTOK_RE.match(co or \"\"):\n",
    "            rows.append([brand, desc, tar, nic, co])\n",
    "    return rows\n",
    "\n",
    "def leading_brand_fix(brand, desc):\n",
    "    \"\"\"\n",
    "    Move leading brand words from desc into brand until a characteristic token appears.\n",
    "    Handles cases like:\n",
    "      brand='Benson &' desc='& Hedges King F HP'  -> 'Benson & Hedges' + 'King F HP'\n",
    "      brand='Best'     desc='Buy* 100 F HP'       -> 'Best Buy*'      + '100 F HP'\n",
    "      brand='All'      desc='American Value 100'  -> 'All American Value' + '100'\n",
    "    \"\"\"\n",
    "    if not desc:\n",
    "        return brand, desc, 0\n",
    "    tokens = desc.split()\n",
    "    moved = []\n",
    "    star = 1 if (\"*\" in brand) else 0\n",
    "\n",
    "    # move '& Something' or other capitalized words until a characteristic token\n",
    "    while tokens:\n",
    "        t = tokens[0]\n",
    "        upper = re.sub(r'[^A-Za-z0-9&-]', '', t).upper()\n",
    "        # stop if token is obviously a characteristic or number (e.g., 100)\n",
    "        if upper in CHAR_STARTERS or NUMTOK_RE.match(t):\n",
    "            break\n",
    "        # likely part of brand -> move it\n",
    "        moved.append(tokens.pop(0))\n",
    "\n",
    "    if moved:\n",
    "        brand = (brand + \" \" + \" \".join(moved)).strip()\n",
    "\n",
    "    # if desc started with \"& Hedges\" and brand was \"Benson &\", above will move \"&\" and \"Hedges\"\n",
    "    # remove any '*' that came with moved piece\n",
    "    if \"*\" in brand:\n",
    "        star = 1\n",
    "        brand = brand.replace(\"*\",\"\").strip()\n",
    "\n",
    "    desc = \" \".join(tokens).strip()\n",
    "    return brand, desc, star\n",
    "\n",
    "def parse_characteristics(desc):\n",
    "    d = (desc or \"\").strip()\n",
    "\n",
    "    # Size with no missing: prefer 100, then King; otherwise Reg\n",
    "    if re.search(r'(?<!\\d)100(?!\\d)', d):\n",
    "        size = \"100\"\n",
    "    elif re.search(r'(?i)\\bKing(s)?\\b', d):\n",
    "        size = \"King\"\n",
    "    else:\n",
    "        size = \"Reg\"\n",
    "\n",
    "    # Dummies\n",
    "    F  = 1 if re.search(r'(?i)(^|[^\\w])F($|[^\\w])|Full\\s*Filter', d) else 0\n",
    "    NF = 1 if re.search(r'(?i)(^|[^\\w])NF($|[^\\w])|Non[-\\s]*Filter', d) else 0\n",
    "    HP = 1 if re.search(r'(?i)\\bHP\\b|Hard\\s*Pack', d) else 0\n",
    "    SP = 1 if re.search(r'(?i)\\bSP\\b|Soft\\s*Pack', d) else 0\n",
    "\n",
    "    ULTRA = 1 if re.search(r'(?i)Ultra(?:-|\\s*)Lt|Ultra Light|Ultima', d) else 0\n",
    "    LT    = 1 if (not ULTRA) and re.search(r'(?i)\\bLt\\b|Light', d) else 0\n",
    "    FF    = 1 if re.search(r'(?i)\\bFF\\b|Full\\s*Flavor', d) else 0\n",
    "\n",
    "    MENTHOL = 1 if re.search(r'(?i)Menthol', d) else 0\n",
    "    GENERIC = 1 if re.search(r'(?i)\\bGeneric\\b', d) else 0\n",
    "    DLX     = 1 if re.search(r'(?i)\\bDLX\\b|Deluxe', d) else 0\n",
    "    SPECIAL = 1 if re.search(r'(?i)\\bSpecial\\b', d) else 0\n",
    "    # \"Sup Slim\" / \"Super Slim\" / \"Superslim\" / \"Slim\"\n",
    "    SUPSLIM = 1 if re.search(r'(?i)Sup(?:er)?\\s*Slim|Superslim|Super[-\\s]*Slim', d) else 0\n",
    "    SLIM    = 1 if (not SUPSLIM) and re.search(r'(?i)\\bSlim(s)?\\b', d) else 0\n",
    "\n",
    "    return size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM\n",
    "\n",
    "def split_op_val(s):\n",
    "    s = (s or \"\").strip()\n",
    "    op = \"<\" if s.startswith(\"<\") else \"\"\n",
    "    val = s[1:] if op == \"<\" else s\n",
    "    return op, val\n",
    "\n",
    "all_rows = []\n",
    "with pdfplumber.open(PDF) as pdf:\n",
    "    for pno in range(PAGE_START, PAGE_END + 1):\n",
    "        page = pdf.pages[pno-1]\n",
    "        text = page.extract_text() or \"\"\n",
    "        if not text or not HDR_RE.search(text):\n",
    "            continue\n",
    "\n",
    "        cuts, header_top, header_bottom = find_header_cuts(page)\n",
    "        if not cuts:\n",
    "            continue\n",
    "\n",
    "        words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "        page_rows = bucketize(words, cuts, header_bottom)\n",
    "        all_rows.extend(page_rows)\n",
    "\n",
    "cleaned = []\n",
    "for brand, desc, Tar, Nic, CO in all_rows:\n",
    "    if not (brand or desc):\n",
    "        continue\n",
    "    # strip spaces\n",
    "    brand = (brand or \"\").strip()\n",
    "    desc  = (desc  or \"\").strip()\n",
    "\n",
    "    # move brand fragments from desc -> brand (Benson & | & Hedges, Best | Buy*, etc.)\n",
    "    brand, desc, star_from_move = leading_brand_fix(brand, desc)\n",
    "\n",
    "    # manufacturer-tested star: from brand OR anything we moved\n",
    "    manufacturer_tested = 1 if (\"*\" in brand or star_from_move) else 0\n",
    "    brand = brand.replace(\"*\",\"\").strip()\n",
    "\n",
    "    # drop any header echoes\n",
    "    if re.match(r'(?i)^brand\\s*name$', brand) or re.match(r'(?i)^description$', desc):\n",
    "        continue\n",
    "\n",
    "    # parse characteristics (ensure Size has no missing: 100/King/Reg)\n",
    "    size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM = parse_characteristics(desc)\n",
    "\n",
    "    # TNCO split\n",
    "    Tar_op, Tar_val = split_op_val(Tar)\n",
    "    Nic_op, Nic_val = split_op_val(Nic)\n",
    "    CO_op,  CO_val  = split_op_val(CO)\n",
    "\n",
    "    # Value-brand flag\n",
    "    value_brand = 1 if brand.upper() in VALUE_BRANDS else 0\n",
    "\n",
    "    cleaned.append([\n",
    "        brand, desc, size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM,\n",
    "        Tar, Nic, CO, Tar_op, Tar_val, Nic_op, Nic_val, CO_op, CO_val, manufacturer_tested, value_brand\n",
    "    ])\n",
    "\n",
    "# write CSV\n",
    "os.makedirs(os.path.dirname(os.path.abspath(OUT)), exist_ok=True)\n",
    "with open(OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\n",
    "        \"Brand Name\",\"Description\",\"Size\",\"F\",\"NF\",\"HP\",\"SP\",\"Ultra_Lt\",\"Lt\",\"FF\",\"Menthol\",\n",
    "        \"Generic\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\n",
    "        \"Tar\",\"Nic\",\"CO\",\"Tar_op\",\"Tar_value\",\"Nic_op\",\"Nic_value\",\"CO_op\",\"CO_value\",\n",
    "        \"manufacturer_tested\",\"value_brand\"\n",
    "    ])\n",
    "    w.writerows(cleaned)\n",
    "\n",
    "print(f\"Wrote {OUT} with {len(cleaned)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ff6c2-6239-45bd-9a2b-d7042f289cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===== 1) Set your PS2 root once =====\n",
    "ROOT = Path(r\"\\ECON847\")  # <- change if needed\n",
    "\n",
    "UPC_PATH = ROOT / \"upccig.csv\"\n",
    "FTC_PATH = ROOT / \"ftc_1998_tnco_clean.csv\"\n",
    "OUT_DIR  = FTC_PATH.parent\n",
    "\n",
    "print(\"Reading UPC  :\", UPC_PATH)\n",
    "print(\"Reading FTC  :\", FTC_PATH)\n",
    "print(\"Saving to    :\", OUT_DIR)\n",
    "\n",
    "# ===== 2) Helpers =====\n",
    "def norm(s: str) -> str:\n",
    "    s = (s or \"\").upper()\n",
    "    s = re.sub(r\"[^\\w&]+\", \" \", s)        # keep letters/digits/_/&\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def parse_size(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    if re.search(r\"(?<!\\d)100(?!\\d)|\\b100S?\\b\", t): return \"100\"\n",
    "    if re.search(r\"\\bKING(S)?\\b\", t):               return \"King\"\n",
    "    return \"Reg\"  # no missing\n",
    "\n",
    "def parse_pack(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    if re.search(r\"\\b(HP|HARD\\s*PACK|BOX|BOXES)\\b\", t): return \"HP\"\n",
    "    if re.search(r\"\\b(SP|SOFT\\s*PACK)\\b\", t):           return \"SP\"\n",
    "    return \"UNK\"  # fall back to brand+size\n",
    "\n",
    "def parse_char_dummies(text: str):\n",
    "    t = norm(text)\n",
    "    menthol = 1 if \"MENTHOL\" in t else 0\n",
    "    dlx     = 1 if re.search(r\"\\b(DLX|DELUXE)\\b\", t) else 0\n",
    "    special = 1 if re.search(r\"\\bSPECIAL\\b\", t) else 0\n",
    "    supslim = 1 if re.search(r\"\\b(SUPER\\s*SLIM|SUPSLIM|SUPERSLIM)\\b\", t) else 0\n",
    "    slim    = 1 if (not supslim) and re.search(r\"\\bSLIM(S)?\\b\", t) else 0\n",
    "    generic = 1 if re.search(r\"\\bGENERIC\\b\", t) else 0\n",
    "    carton  = 1 if re.search(r\"\\bCARTON(S)?\\b\", t) else 0\n",
    "    single  = 1 if re.search(r\"\\bSINGLE\\b\", t) else 0\n",
    "    pack_kw = 1 if re.search(r\"\\bPACK\\b|\\bPK\\b\", t) else 0\n",
    "    return menthol, dlx, special, supslim, slim, generic, carton, single, pack_kw\n",
    "\n",
    "def safe_float(x):\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s: return None\n",
    "    try: return float(s)\n",
    "    except:\n",
    "        try: return float(s.replace(\",\", \"\"))\n",
    "        except: return None\n",
    "\n",
    "# ---- brand lexicon from FTC (longest match, read with errors='replace') ----\n",
    "def build_brand_patterns(ftc_csv_path: Path):\n",
    "    brands = set()\n",
    "    with ftc_csv_path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            b = (row.get(\"Brand Name\") or \"\").strip()\n",
    "            if b: brands.add(b)\n",
    "    # a couple common value-brand aliases\n",
    "    brands |= {\"ALL AMERICAN VALUE\", \"AMERICAN VALUE\", \"ALL-AMERICAN VALUE\"}\n",
    "\n",
    "    pats = []\n",
    "    for b in brands:\n",
    "        tokens = [re.escape(x) for x in norm(b).split()]\n",
    "        if tokens:\n",
    "            pats.append((b, re.compile(r\"\\b\" + r\"\\s*\".join(tokens) + r\"\\b\")))\n",
    "    pats.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    return pats\n",
    "\n",
    "def find_brand(descrip: str, brand_patterns):\n",
    "    T = norm(descrip)\n",
    "    for brand, pat in brand_patterns:\n",
    "        if pat.search(T):\n",
    "            return brand\n",
    "    return \"\"\n",
    "\n",
    "# ---- FTC aggregates by (brand,size,pack) → (brand,size) → (brand) ----\n",
    "def load_ftc_aggregates(ftc_csv_path: Path):\n",
    "    b_sp = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "    b_s  = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "    b    = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "\n",
    "    with ftc_csv_path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        has_val = {\"Tar_value\",\"Nic_value\",\"CO_value\"}.issubset(r.fieldnames or [])\n",
    "        for row in r:\n",
    "            brand = (row.get(\"Brand Name\") or \"\").strip()\n",
    "            if not brand: \n",
    "                continue\n",
    "            size  = (row.get(\"Size\") or \"Reg\").strip()\n",
    "\n",
    "            hp = str(row.get(\"HP\") or \"\").strip()\n",
    "            sp = str(row.get(\"SP\") or \"\").strip()\n",
    "            pack = \"HP\" if hp in (\"1\",\"True\",\"TRUE\",\"true\") else (\"SP\" if sp in (\"1\",\"True\",\"TRUE\",\"true\") else \"UNK\")\n",
    "\n",
    "            if has_val:\n",
    "                tar = safe_float(row.get(\"Tar_value\"))\n",
    "                nic = safe_float(row.get(\"Nic_value\"))\n",
    "                co  = safe_float(row.get(\"CO_value\"))\n",
    "            else:\n",
    "                def num_from_raw(v):\n",
    "                    s = (v or \"\").strip()\n",
    "                    s = s[1:] if s.startswith(\"<\") else s\n",
    "                    return safe_float(s)\n",
    "                tar = num_from_raw(row.get(\"Tar\"))\n",
    "                nic = num_from_raw(row.get(\"Nic\"))\n",
    "                co  = num_from_raw(row.get(\"CO\"))\n",
    "\n",
    "            for name, val in ((\"tar\",tar), (\"nic\",nic), (\"co\",co)):\n",
    "                if val is None: \n",
    "                    continue\n",
    "                b_sp[(brand,size,pack)][name].append(val)\n",
    "                b_s[(brand,size)][name].append(val)\n",
    "                b[(brand,)][name].append(val)\n",
    "\n",
    "    def reduce_means(B):\n",
    "        out = {}\n",
    "        for k, vv in B.items():\n",
    "            def m(x): return sum(x)/len(x) if x else None\n",
    "            out[k] = (m(vv[\"tar\"]), m(vv[\"nic\"]), m(vv[\"co\"]))\n",
    "        return out\n",
    "\n",
    "    return reduce_means(b_sp), reduce_means(b_s), reduce_means(b)\n",
    "\n",
    "# ===== 3) Clean & merge (reads UPC with errors='replace') =====\n",
    "def clean_and_merge(UPC_PATH: Path, FTC_PATH: Path, OUT_DIR: Path):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_clean = OUT_DIR / \"upccig_clean.csv\"\n",
    "    out_merge = OUT_DIR / \"upccig_with_tnco.csv\"\n",
    "\n",
    "    brand_patterns = build_brand_patterns(FTC_PATH)\n",
    "    by_bsp, by_bs, by_b = load_ftc_aggregates(FTC_PATH)\n",
    "\n",
    "    with UPC_PATH.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as fin, \\\n",
    "         out_clean.open(\"w\", newline=\"\", encoding=\"utf-8\") as fclean, \\\n",
    "         out_merge.open(\"w\", newline=\"\", encoding=\"utf-8\") as fmerge:\n",
    "\n",
    "        rin = csv.DictReader(fin)\n",
    "        cols_in = rin.fieldnames or []\n",
    "        clean_cols = cols_in + [\"brand\",\"size\",\"pack\",\"Menthol\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\"Generic\",\"Carton\",\"Single\",\"Pack_kw\"]\n",
    "        rout_clean = csv.DictWriter(fclean, fieldnames=clean_cols)\n",
    "        rout_clean.writeheader()\n",
    "\n",
    "        merge_cols = clean_cols + [\"tnco_key_level\",\"Tar_mean\",\"Nic_mean\",\"CO_mean\"]\n",
    "        rout_merge = csv.DictWriter(fmerge, fieldnames=merge_cols)\n",
    "        rout_merge.writeheader()\n",
    "\n",
    "        n, matched = 0, 0\n",
    "        for row in rin:\n",
    "            n += 1\n",
    "            descr = (row.get(\"DESCRIP\") or row.get(\"DESCR\") or \"\").strip()\n",
    "\n",
    "            brand = find_brand(descr, brand_patterns) or \"\"   # NA if not found\n",
    "            size  = parse_size(descr)\n",
    "            pack  = parse_pack(descr)                         # HP / SP / UNK\n",
    "\n",
    "            Menthol, Dlx, Special, SupSlim, Slim, Generic, Carton, Single, Pack_kw = parse_char_dummies(descr)\n",
    "\n",
    "            clean_row = dict(row)\n",
    "            clean_row.update({\n",
    "                \"brand\": brand if brand else \"NA\",\n",
    "                \"size\": size,\n",
    "                \"pack\": pack,\n",
    "                \"Menthol\": Menthol, \"Dlx\": Dlx, \"Special\": Special,\n",
    "                \"SupSlim\": SupSlim, \"Slim\": Slim, \"Generic\": Generic,\n",
    "                \"Carton\": Carton, \"Single\": Single, \"Pack_kw\": Pack_kw\n",
    "            })\n",
    "            rout_clean.writerow(clean_row)\n",
    "\n",
    "            # Merge priority: (brand,size,pack) → (brand,size) → (brand)\n",
    "            tnco_key_level = \"none\"\n",
    "            Tar_mean = Nic_mean = CO_mean = \"\"\n",
    "\n",
    "            if brand:\n",
    "                if pack in (\"HP\",\"SP\") and (brand,size,pack) in by_bsp:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_bsp[(brand,size,pack)]; tnco_key_level = \"brand+size+pack\"\n",
    "                elif (brand,size) in by_bs:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_bs[(brand,size)]; tnco_key_level = \"brand+size\"\n",
    "                elif (brand,) in by_b:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_b[(brand,)]; tnco_key_level = \"brand\"\n",
    "\n",
    "            matched += (tnco_key_level != \"none\")\n",
    "\n",
    "            merge_row = dict(clean_row)\n",
    "            merge_row.update({\n",
    "                \"tnco_key_level\": tnco_key_level,\n",
    "                \"Tar_mean\": f\"{Tar_mean:.3f}\" if isinstance(Tar_mean,float) else \"\",\n",
    "                \"Nic_mean\": f\"{Nic_mean:.3f}\" if isinstance(Nic_mean,float) else \"\",\n",
    "                \"CO_mean\":  f\"{CO_mean:.3f}\"  if isinstance(CO_mean, float) else \"\",\n",
    "            })\n",
    "            rout_merge.writerow(merge_row)\n",
    "\n",
    "    print(f\"Wrote: {out_clean}\")\n",
    "    print(f\"Wrote: {out_merge}\")\n",
    "    print(f\"Rows: {n} | matched to FTC at any level: {matched}\")\n",
    "\n",
    "# run\n",
    "clean_and_merge(UPC_PATH, FTC_PATH, OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
