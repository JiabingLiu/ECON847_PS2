{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8835ad0-1649-4d0f-b01a-05186d60d505",
   "metadata": {},
   "source": [
    "# $\\text{Get PDF on Server}$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed4e605a-9aef-4dc9-8851-1d44f6c8fa28",
   "metadata": {},
   "source": [
    "cd ~/ECON847\n",
    "wget --user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\" \\\n",
    "     -O 1998tarnicotinereport_0.pdf \\\n",
    "     \"https://www.ftc.gov/sites/default/files/documents/reports/2000-report-tar-nicotine-and-carbon-monoxide-covering-1998/1998tarnicotinereport_0.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79673e3-376d-4421-9a9f-ed69f7aa2a3b",
   "metadata": {},
   "source": [
    "# $\\text{PDF Cleaning}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a6436-0b02-4603-9afb-7c80669a809d",
   "metadata": {},
   "source": [
    "We extract cigrarettes product characteristics from [Federal Trade Commission Report of Cigarettes (1998)](https://www.ftc.gov/sites/default/files/documents/reports/2000-report-tar-nicotine-and-carbon-monoxide-covering-1998/1998tarnicotinereport_0.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1cdafec5-bf16-4231-8852-66f79b852b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Characteristics/ftc_1998_tnco_clean.csv with 1333 rows\n"
     ]
    }
   ],
   "source": [
    "# save as extract_ftc_tnco_v2.py\n",
    "# run: python extract_ftc_tnco_v2.py /path/to/1998tarnicotinereport_0.pdf\n",
    "import sys, re, csv, os\n",
    "import pdfplumber\n",
    "\n",
    "PDF = \"Characteristics/1998tarnicotinereport_0.pdf\"\n",
    "OUT = \"Characteristics/ftc_1998_tnco_clean.csv\"\n",
    "PAGE_START, PAGE_END = 10, 45  # adjust if needed\n",
    "\n",
    "HDR_RE    = re.compile(r'(?i)\\bbrand\\s+name\\b.*\\bdescription\\b.*\\btar\\b.*\\bnic\\b.*\\bco\\b')\n",
    "NUMTOK_RE = re.compile(r'^(?:<)?\\d+(?:\\.\\d+)?$|^NA$', re.I)\n",
    "\n",
    "# Known tokens that indicate we've reached characteristics (not brand words)\n",
    "SIZE_TOKS   = {\"100\",\"100s\",\"100's\",\"KING\",\"KINGS\",\"REG\",\"REGULAR\"}\n",
    "PACK_TOKS   = {\"HP\",\"SP\"}  # hard/soft pack abbreviations used in the report\n",
    "FILTER_TOKS = {\"F\",\"NF\"}\n",
    "STRENGTH_TOKS = {\"FF\",\"FULL\",\"FLAVOR\",\"LIGHT\",\"LT\",\"ULTRA\",\"ULTRA-LT\",\"ULTIMA\",\"MILD\"}\n",
    "OTHER_CHAR_TOKS = {\"MENTHOL\",\"DLX\",\"DELUXE\",\"SPECIAL\",\"SUP\",\"SLIM\",\"SUPERSLIM\",\"SUPER\",\"BOX\",\"BOXES\",\"GENERIC\"}\n",
    "\n",
    "CHAR_STARTERS = SIZE_TOKS | PACK_TOKS | FILTER_TOKS | STRENGTH_TOKS | OTHER_CHAR_TOKS\n",
    "\n",
    "# Value/generic-style brands to flag (you can extend this)\n",
    "VALUE_BRANDS = {\n",
    "    \"BEST BUY\", \"BARGAIN BUY\", \"ALL AMERICAN VALUE\", \"ALL-AMERICAN VALUE\",\n",
    "    \"AMERICAN VALUE\"\n",
    "}\n",
    "\n",
    "def find_header_cuts(page):\n",
    "    words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "    header_words = [w for w in words if w[\"text\"].lower() in {\"brand\",\"name\",\"description\",\"tar\",\"nic\",\"co\"}]\n",
    "    if not header_words:\n",
    "        return None, None, None\n",
    "\n",
    "    # group header words by line\n",
    "    y_key = round(sorted(header_words, key=lambda w: w[\"top\"])[0][\"top\"]/3.0)\n",
    "    ws = [w for w in header_words if round(w[\"top\"]/3.0)==y_key]\n",
    "\n",
    "    # x starts\n",
    "    def x(label):\n",
    "        xs = [w[\"x0\"] for w in ws if w[\"text\"].lower()==label]\n",
    "        return min(xs) if xs else None\n",
    "\n",
    "    x_brand = x(\"brand\"); x_name = x(\"name\"); x_desc = x(\"description\")\n",
    "    x_tar   = x(\"tar\");   x_nic  = x(\"nic\");  x_co   = x(\"co\")\n",
    "    if None in (x_brand,x_name,x_desc,x_tar,x_nic,x_co):\n",
    "        return None, None, None\n",
    "\n",
    "    x_brand_start = min(x_brand, x_name)\n",
    "    x_brand_end   = (x_desc + x_brand_start)/2.0\n",
    "    x_desc_end    = (x_tar + x_desc)/2.0\n",
    "    x_tar_end     = (x_nic + x_tar)/2.0\n",
    "    x_nic_end     = (x_co + x_nic)/2.0\n",
    "\n",
    "    header_top    = min(w[\"top\"] for w in ws)\n",
    "    header_bottom = max(w[\"bottom\"] for w in ws)\n",
    "    return (x_brand_start, x_brand_end, x_desc_end, x_tar_end, x_nic_end), header_top, header_bottom\n",
    "\n",
    "def bucketize(words, cuts, header_bottom):\n",
    "    x_brand_start, x_brand_end, x_desc_end, x_tar_end, x_nic_end = cuts\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        y_mid = (w[\"top\"] + w[\"bottom\"]) / 2.0\n",
    "        if y_mid <= header_bottom + 1.0:\n",
    "            continue\n",
    "        key = round(y_mid / 2.0, 1)\n",
    "        lines.setdefault(key, []).append(w)\n",
    "\n",
    "    rows = []\n",
    "    for key in sorted(lines.keys()):\n",
    "        ws = sorted(lines[key], key=lambda z: z[\"x0\"])\n",
    "        cols = {\"brand\": [], \"desc\": [], \"tar\": [], \"nic\": [], \"co\": []}\n",
    "        for w in ws:\n",
    "            xm = (w[\"x0\"] + w[\"x1\"]) / 2.0\n",
    "            txt = w[\"text\"]\n",
    "            if xm < x_brand_end:\n",
    "                cols[\"brand\"].append(txt)\n",
    "            elif xm < x_desc_end:\n",
    "                cols[\"desc\"].append(txt)\n",
    "            elif xm < x_tar_end:\n",
    "                cols[\"tar\"].append(txt)\n",
    "            elif xm < x_nic_end:\n",
    "                cols[\"nic\"].append(txt)\n",
    "            else:\n",
    "                cols[\"co\"].append(txt)\n",
    "\n",
    "        brand = \" \".join(cols[\"brand\"]).strip()\n",
    "        desc  = \" \".join(cols[\"desc\"]).strip()\n",
    "        tar   = \" \".join(cols[\"tar\"]).strip()\n",
    "        nic   = \" \".join(cols[\"nic\"]).strip()\n",
    "        co    = \" \".join(cols[\"co\"]).strip()\n",
    "\n",
    "        # If TNCO aren't clean, try right-anchored salvage\n",
    "        if not (NUMTOK_RE.match(tar or \"\") and NUMTOK_RE.match(nic or \"\") and NUMTOK_RE.match(co or \"\")):\n",
    "            toks = (brand + \"  \" + desc + \"  \" + tar + \" \" + nic + \" \" + co).split()\n",
    "            idxs = [i for i,t in enumerate(toks) if NUMTOK_RE.match(t)]\n",
    "            if len(idxs) >= 3:\n",
    "                i3,i2,i1 = idxs[-1],idxs[-2],idxs[-3]\n",
    "                co,nic,tar = toks[i3], toks[i2], toks[i1]\n",
    "                left = \" \".join(toks[:i1])\n",
    "                m = re.search(r'\\s{2,}', left)\n",
    "                if m:\n",
    "                    brand = left[:m.start()].strip()\n",
    "                    desc  = left[m.end():].strip()\n",
    "                else:\n",
    "                    parts = left.split()\n",
    "                    brand = parts[0] if parts else \"\"\n",
    "                    desc  = \" \".join(parts[1:]) if len(parts)>1 else \"\"\n",
    "\n",
    "        if NUMTOK_RE.match(tar or \"\") and NUMTOK_RE.match(nic or \"\") and NUMTOK_RE.match(co or \"\"):\n",
    "            rows.append([brand, desc, tar, nic, co])\n",
    "    return rows\n",
    "\n",
    "def leading_brand_fix(brand, desc):\n",
    "    \"\"\"\n",
    "    Move leading brand words from desc into brand until a characteristic token appears.\n",
    "    Handles cases like:\n",
    "      brand='Benson &' desc='& Hedges King F HP'  -> 'Benson & Hedges' + 'King F HP'\n",
    "      brand='Best'     desc='Buy* 100 F HP'       -> 'Best Buy*'      + '100 F HP'\n",
    "      brand='All'      desc='American Value 100'  -> 'All American Value' + '100'\n",
    "    \"\"\"\n",
    "    if not desc:\n",
    "        return brand, desc, 0\n",
    "    tokens = desc.split()\n",
    "    moved = []\n",
    "    star = 1 if (\"*\" in brand) else 0\n",
    "\n",
    "    # move '& Something' or other capitalized words until a characteristic token\n",
    "    while tokens:\n",
    "        t = tokens[0]\n",
    "        upper = re.sub(r'[^A-Za-z0-9&-]', '', t).upper()\n",
    "        # stop if token is obviously a characteristic or number (e.g., 100)\n",
    "        if upper in CHAR_STARTERS or NUMTOK_RE.match(t):\n",
    "            break\n",
    "        # likely part of brand -> move it\n",
    "        moved.append(tokens.pop(0))\n",
    "\n",
    "    if moved:\n",
    "        brand = (brand + \" \" + \" \".join(moved)).strip()\n",
    "\n",
    "    # if desc started with \"& Hedges\" and brand was \"Benson &\", above will move \"&\" and \"Hedges\"\n",
    "    # remove any '*' that came with moved piece\n",
    "    if \"*\" in brand:\n",
    "        star = 1\n",
    "        brand = brand.replace(\"*\",\"\").strip()\n",
    "\n",
    "    desc = \" \".join(tokens).strip()\n",
    "    return brand, desc, star\n",
    "\n",
    "def parse_characteristics(desc):\n",
    "    d = (desc or \"\").strip()\n",
    "\n",
    "    # Size with no missing: prefer 100, then King; otherwise Reg\n",
    "    if re.search(r'(?<!\\d)100(?!\\d)', d):\n",
    "        size = \"100\"\n",
    "    elif re.search(r'(?i)\\bKing(s)?\\b', d):\n",
    "        size = \"King\"\n",
    "    else:\n",
    "        size = \"Reg\"\n",
    "\n",
    "    # Dummies\n",
    "    F  = 1 if re.search(r'(?i)(^|[^\\w])F($|[^\\w])|Full\\s*Filter', d) else 0\n",
    "    NF = 1 if re.search(r'(?i)(^|[^\\w])NF($|[^\\w])|Non[-\\s]*Filter', d) else 0\n",
    "    HP = 1 if re.search(r'(?i)\\bHP\\b|Hard\\s*Pack', d) else 0\n",
    "    SP = 1 if re.search(r'(?i)\\bSP\\b|Soft\\s*Pack', d) else 0\n",
    "\n",
    "    ULTRA = 1 if re.search(r'(?i)Ultra(?:-|\\s*)Lt|Ultra Light|Ultima', d) else 0\n",
    "    LT    = 1 if (not ULTRA) and re.search(r'(?i)\\bLt\\b|Light', d) else 0\n",
    "    FF    = 1 if re.search(r'(?i)\\bFF\\b|Full\\s*Flavor', d) else 0\n",
    "\n",
    "    MENTHOL = 1 if re.search(r'(?i)Menthol', d) else 0\n",
    "    GENERIC = 1 if re.search(r'(?i)\\bGeneric\\b', d) else 0\n",
    "    DLX     = 1 if re.search(r'(?i)\\bDLX\\b|Deluxe', d) else 0\n",
    "    SPECIAL = 1 if re.search(r'(?i)\\bSpecial\\b', d) else 0\n",
    "    # \"Sup Slim\" / \"Super Slim\" / \"Superslim\" / \"Slim\"\n",
    "    SUPSLIM = 1 if re.search(r'(?i)Sup(?:er)?\\s*Slim|Superslim|Super[-\\s]*Slim', d) else 0\n",
    "    SLIM    = 1 if (not SUPSLIM) and re.search(r'(?i)\\bSlim(s)?\\b', d) else 0\n",
    "\n",
    "    return size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM\n",
    "\n",
    "def split_op_val(s):\n",
    "    s = (s or \"\").strip()\n",
    "    op = \"<\" if s.startswith(\"<\") else \"\"\n",
    "    val = s[1:] if op == \"<\" else s\n",
    "    return op, val\n",
    "\n",
    "all_rows = []\n",
    "with pdfplumber.open(PDF) as pdf:\n",
    "    for pno in range(PAGE_START, PAGE_END + 1):\n",
    "        page = pdf.pages[pno-1]\n",
    "        text = page.extract_text() or \"\"\n",
    "        if not text or not HDR_RE.search(text):\n",
    "            continue\n",
    "\n",
    "        cuts, header_top, header_bottom = find_header_cuts(page)\n",
    "        if not cuts:\n",
    "            continue\n",
    "\n",
    "        words = page.extract_words(use_text_flow=True, keep_blank_chars=False)\n",
    "        page_rows = bucketize(words, cuts, header_bottom)\n",
    "        all_rows.extend(page_rows)\n",
    "\n",
    "cleaned = []\n",
    "for brand, desc, Tar, Nic, CO in all_rows:\n",
    "    if not (brand or desc):\n",
    "        continue\n",
    "    # strip spaces\n",
    "    brand = (brand or \"\").strip()\n",
    "    desc  = (desc  or \"\").strip()\n",
    "\n",
    "    # move brand fragments from desc -> brand (Benson & | & Hedges, Best | Buy*, etc.)\n",
    "    brand, desc, star_from_move = leading_brand_fix(brand, desc)\n",
    "\n",
    "    # manufacturer-tested star: from brand OR anything we moved\n",
    "    manufacturer_tested = 1 if (\"*\" in brand or star_from_move) else 0\n",
    "    brand = brand.replace(\"*\",\"\").strip()\n",
    "\n",
    "    # drop any header echoes\n",
    "    if re.match(r'(?i)^brand\\s*name$', brand) or re.match(r'(?i)^description$', desc):\n",
    "        continue\n",
    "\n",
    "    # parse characteristics (ensure Size has no missing: 100/King/Reg)\n",
    "    size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM = parse_characteristics(desc)\n",
    "\n",
    "    # TNCO split\n",
    "    Tar_op, Tar_val = split_op_val(Tar)\n",
    "    Nic_op, Nic_val = split_op_val(Nic)\n",
    "    CO_op,  CO_val  = split_op_val(CO)\n",
    "\n",
    "    # Value-brand flag\n",
    "    value_brand = 1 if brand.upper() in VALUE_BRANDS else 0\n",
    "\n",
    "    cleaned.append([\n",
    "        brand, desc, size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM,\n",
    "        Tar, Nic, CO, Tar_op, Tar_val, Nic_op, Nic_val, CO_op, CO_val, manufacturer_tested, value_brand\n",
    "    ])\n",
    "\n",
    "# write CSV\n",
    "os.makedirs(os.path.dirname(os.path.abspath(OUT)), exist_ok=True)\n",
    "with open(OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\n",
    "        \"Brand Name\",\"Description\",\"Size\",\"F\",\"NF\",\"HP\",\"SP\",\"Ultra_Lt\",\"Lt\",\"FF\",\"Menthol\",\n",
    "        \"Generic\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\n",
    "        \"Tar\",\"Nic\",\"CO\",\"Tar_op\",\"Tar_value\",\"Nic_op\",\"Nic_value\",\"CO_op\",\"CO_value\",\n",
    "        \"manufacturer_tested\",\"value_brand\"\n",
    "    ])\n",
    "    w.writerows(cleaned)\n",
    "\n",
    "print(f\"Wrote {OUT} with {len(cleaned)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "86ead8ba-ca96-4d37-9904-6bd65429d1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Size</th>\n",
       "      <th>F</th>\n",
       "      <th>NF</th>\n",
       "      <th>HP</th>\n",
       "      <th>SP</th>\n",
       "      <th>Ultra_Lt</th>\n",
       "      <th>Lt</th>\n",
       "      <th>FF</th>\n",
       "      <th>...</th>\n",
       "      <th>Nic</th>\n",
       "      <th>CO</th>\n",
       "      <th>Tar_op</th>\n",
       "      <th>Tar_value</th>\n",
       "      <th>Nic_op</th>\n",
       "      <th>Nic_value</th>\n",
       "      <th>CO_op</th>\n",
       "      <th>CO_value</th>\n",
       "      <th>manufacturer_tested</th>\n",
       "      <th>value_brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carlton</td>\n",
       "      <td>King F HP Ultra-Lt</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.05</td>\n",
       "      <td>&lt;0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.05</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now</td>\n",
       "      <td>100 F HP</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.05</td>\n",
       "      <td>&lt;0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.05</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now</td>\n",
       "      <td>King F HP</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;0.05</td>\n",
       "      <td>&lt;0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.05</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carlton</td>\n",
       "      <td>King F SP Ultra</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;0.5</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carlton</td>\n",
       "      <td>100 F HP Lt Menthol</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>Worth</td>\n",
       "      <td>King F SP FF Menthol</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>Worth</td>\n",
       "      <td>King F SP Lt</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>Worth</td>\n",
       "      <td>King F SP Lt Menthol</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Worth</td>\n",
       "      <td>King F SP Ultra-Lt</td>\n",
       "      <td>King</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Worth</td>\n",
       "      <td>King NF SP</td>\n",
       "      <td>King</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1333 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Brand Name           Description  Size  F  NF  HP  SP  Ultra_Lt  Lt  FF  \\\n",
       "0       Carlton    King F HP Ultra-Lt  King  1   0   1   0         1   0   0   \n",
       "1           Now              100 F HP   100  1   0   1   0         0   0   0   \n",
       "2           Now             King F HP  King  1   0   1   0         0   0   0   \n",
       "3       Carlton       King F SP Ultra  King  1   0   0   1         0   0   0   \n",
       "4       Carlton   100 F HP Lt Menthol   100  1   0   1   0         0   1   0   \n",
       "...         ...                   ...   ... ..  ..  ..  ..       ...  ..  ..   \n",
       "1328      Worth  King F SP FF Menthol  King  1   0   0   1         0   0   1   \n",
       "1329      Worth          King F SP Lt  King  1   0   0   1         0   1   0   \n",
       "1330      Worth  King F SP Lt Menthol  King  1   0   0   1         0   1   0   \n",
       "1331      Worth    King F SP Ultra-Lt  King  1   0   0   1         1   0   0   \n",
       "1332      Worth            King NF SP  King  0   1   0   1         0   0   0   \n",
       "\n",
       "      ...    Nic    CO  Tar_op  Tar_value  Nic_op  Nic_value CO_op CO_value  \\\n",
       "0     ...  <0.05  <0.5       <        0.5       <       0.05     <      0.5   \n",
       "1     ...  <0.05  <0.5       <        0.5       <       0.05     <      0.5   \n",
       "2     ...  <0.05  <0.5       <        0.5       <       0.05     <      0.5   \n",
       "3     ...    0.1  <0.5       <        0.5     NaN       0.10     <      0.5   \n",
       "4     ...    0.1     1       <        0.5     NaN       0.10   NaN      1.0   \n",
       "...   ...    ...   ...     ...        ...     ...        ...   ...      ...   \n",
       "1328  ...    0.8    17     NaN       14.0     NaN       0.80   NaN     17.0   \n",
       "1329  ...    0.7    13     NaN       10.0     NaN       0.70   NaN     13.0   \n",
       "1330  ...    0.6    12     NaN        9.0     NaN       0.60   NaN     12.0   \n",
       "1331  ...    0.3     6     NaN        4.0     NaN       0.30   NaN      6.0   \n",
       "1332  ...    1.1    16     NaN       20.0     NaN       1.10   NaN     16.0   \n",
       "\n",
       "     manufacturer_tested value_brand  \n",
       "0                      0           0  \n",
       "1                      0           0  \n",
       "2                      0           0  \n",
       "3                      1           0  \n",
       "4                      0           0  \n",
       "...                  ...         ...  \n",
       "1328                   1           0  \n",
       "1329                   1           0  \n",
       "1330                   1           0  \n",
       "1331                   1           0  \n",
       "1332                   1           0  \n",
       "\n",
       "[1333 rows x 27 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alt = pd.read_csv(\"Characteristics/ftc_1998_tnco_clean.csv\",encoding=\"cp1252\")\n",
    "alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4083-3f1d-4431-ba87-39f02251b8fb",
   "metadata": {},
   "source": [
    "# $\\text{HTML Cleaning}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad0af9ea-c464-48e3-9872-b659b1cc3105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading local file Characteristics/Nicotine, Tar, and CO Content of Menthol Cigarette Brands in 1994.htm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<script async=\"\" crossorigin=\"anonymous\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9977171164675277\"></script>\n",
       "<title>\n",
       "Nicotine, Tar, and CO Content of Menthol Cigarette Brands in 1994\n",
       "</title>\n",
       "</head>\n",
       "<body>\n",
       "<center>\n",
       "<script type=\"text/javascript\"><!--\n",
       "google_ad_client = \"pub-9977171164675277\";\n",
       "/* cigms_top */\n",
       "google_ad_slot = \"9441854520\";\n",
       "google_ad_width = 728;\n",
       "google_ad_height = 90;\n",
       "//-->\n",
       "</script>\n",
       "<script src=\"http://pagead2.googlesyndication.com/pagead/show_ads.js\" type=\"text/javascript\">\n",
       "</script>\n",
       "</center>\n",
       "<pre>\n",
       "NICOTINE, TAR, AND CO CONTENT OF DOMESTIC CIGARETTES IN 1994\n",
       "    (Menthol Brands, sorted by nicotine, tar, and CO)\n",
       "\n",
       "  NIC   TAR    CO  BRAND NAME             TYPE\n",
       "  ---   ---   ---  ----------             ----\n",
       "   .1     1     1  CARLTON                100  F  HP LT\n",
       "   .1     1     2  CARLTON                KING F  SP ULTRA-LT\n",
       "   .1     1     2  NOW                    KING F  SP\n",
       "   .2     2     2  CARLTON                100  F  SP LT\n",
       "   .2     2     2  KOOL                   KING F  SP ULTRA\n",
       "   .2     2     3  NOW                    100  F  SP\n",
       "   .3     4     7  STERLING               100  F  SP ULTRA-LT\n",
       "   .4     4     5  MERIT                  KING F  SP ULTRA-LT\n",
       "   .4     5     5  KOOL                   100  F  SP ULTRA\n",
       "   .4     5     6  GPC                    100  F  SP ULTRA-LT\n",
       "   .4     5     6  GPC                    KING F  SP ULTRA-LT\n",
       "   .4     5     7  SALEM                  100  F  SP ULTRA-LT\n",
       "   .5     4     5  TRIUMPH                KING F  SP\n",
       "   .5     5     4  CAPRI                  100  F  HP ULTRA-LT\n",
       "   .5     5     4  CARLTON                120  F  SP LT\n",
       "   .5     5     4  EVE 120                120  F  HP UL\n",
       "   .5     5     5  TRUE                   KING F  SP\n",
       "   .5     5     5  VIRGINIA SLIMS         100  F  HP ULTRA-LT SLIM\n",
       "   .5     5     6  BENSON &amp; HEDGES        100  F  HP ULTRA-LT DLX\n",
       "   .5     5     6  KOOL                   100  F  HP ULTRA-LT\n",
       "   .5     5     7  MERIT                  100  F  SP ULTRA-LT\n",
       "   .5     5     7  SALEM                  KING F  SP ULTRA-LT\n",
       "   .5     6     5  VIRGINIA SLIMS SUPER   100  F  HP SUP-SLIM\n",
       "   .5     7     9  STERLING               100  F  HP SLIM-LT\n",
       "   .6     5     5  CARLTON                100  F  HP SLIM\n",
       "   .6     6     4  EVE 100                100  F  HP UL SLIM\n",
       "   .6     6     5  MISTY SLIMS            100  F  HP ULTRA-LT SLIM\n",
       "   .6     6     6  TRUE                   100  F  SP\n",
       "   .6     6     7  KOOL                   KING F  HP ULTRA-LT\n",
       "   .6     7     8  KOOL                   KING F  SP LT\n",
       "   .6     7     8  KOOL                   KING F  SP LT\n",
       "   .6     7     9  MERIT                  KING F  SP\n",
       "   .6     8     8  JASMINE                100  F  HP LT SLIM\n",
       "   .6     8     9  BELAIR                 100  F  SP\n",
       "   .6     8    10  BEST VALUE             100  F  SP LT\n",
       "   .6     8    10  DORAL                  KING F  SP LT\n",
       "   .6     8    11  MONARCH                100  F  SP LT\n",
       "   .6     8    11  STERLING               100  F  SP LT\n",
       "   .6     8    12  DORAL                  100  F  SP LT\n",
       "   .6     9    12  BEST VALUE             KING F  SP LT\n",
       "   .6     9    12  MONARCH                KING F  SP LT\n",
       "   .7     8     6  EVE 100                100  F  HP LT SLIM\n",
       "   .7     8     8  GPC                    100  F  SP LT\n",
       "   .7     8     9  KOOL                   100  F  SP LT\n",
       "   .7     8     9  KOOL                   100  F  SP LT\n",
       "   .7     8     9  RALEIGH EXTRA          100  F  SP LT\n",
       "   .7     8     9  VANTAGE                KING F  SP\n",
       "   .7     8     9  VIRGINIA SLIMS         100  F  HP LT SLIM\n",
       "   .7     8    10  KENT GOLDEN LIGHTS     KING F  SP LT\n",
       "   .7     8    10  VANTAGE                100  F  SP\n",
       "   .7     8    11  SALEM                  100  F  SP LT\n",
       "   .7     9     6  CAPRI                  100  F  HP SUP-SLIM\n",
       "   .7     9     9  ALPINE                 KING F  HP LT\n",
       "   .7     9     9  GPC                    KING F  SP LT\n",
       "   .7     9     9  MORE                   100  F  HP LT\n",
       "   .7     9    10  ALPINE                 100  F  SP LT\n",
       "   .7     9    10  ALPINE                 KING F  SP LT\n",
       "   .7     9    10  MARLBORO               KING F  HP LT\n",
       "   .7     9    11  CENTURY 25pkg          100  F  SP LT\n",
       "   .7     9    11  MARLBORO               100  F  HP LT\n",
       "   .7     9    11  NEWPORT LIGHTS         KING F  SP LT\n",
       "   .7     9    12  NEWPORT LIGHTS         KING F  HP LT\n",
       "   .7     9    12  STERLING               KING F  SP LT\n",
       "   .7    10    12  BASIC                  KING F  SP LT\n",
       "   .8     9     8  SALEM                  100  F  HP SLIM-LT\n",
       "   .8     9     9  KOOL                   KING F  HP LT\n",
       "   .8     9     9  MISTY SLIMS            100  F  HP LT SLIM\n",
       "   .8     9    10  KENT GOLDEN LIGHTS     100  F  SP LT\n",
       "   .8     9    11  MERIT                  100  F  SP\n",
       "   .8     9    11  NEWPORT LIGHTS         100  F  HP LT\n",
       "   .8     9    15  MORE                   120  F  SP LT\n",
       "   .8    10     9  RALEIGH EXTRA          KING F  SP LT\n",
       "   .8    10    10  BELAIR                 KING F  SP\n",
       "   .8    10    10  KOOL                   100  F  HP LT\n",
       "   .8    10    10  NEWPORT SLIM           100  F  HP SLIM-LT\n",
       "   .8    10    11  BENSON &amp; HEDGES        100  F  HP LT\n",
       "   .8    10    11  BENSON &amp; HEDGES        100  F  SP LT\n",
       "   .8    10    11  NEWPORT LIGHTS         100  F  SP LT\n",
       "   .8    10    11  PLAYERS 25pkg          KING F  SP LT\n",
       "   .8    10    11  PRIME                  KING F  SP LT\n",
       "   .8    10    11  STYLE                  100  F  HP SLIM-LT\n",
       "   .8    10    11  SUMMIT                 KING F  SP LT\n",
       "   .8    10    12  BRISTOL                KING F  SP LT\n",
       "   .8    10    12  CAMBRIDGE              KING F  SP LT\n",
       "   .8    10    12  MONARCH                KING F  HP LT\n",
       "   .8    10    13  BASIC                  100  F  SP LT\n",
       "   .8    10    13  BRISTOL                100  F  SP LT\n",
       "   .8    10    13  CAMBRIDGE              100  F  SP LT\n",
       "   .8    11    11  KOOL                   KING F  SP MILD\n",
       "   .8    11    11  PLAYERS                KING F  HP\n",
       "   .8    11    13  PLAYERS 25pkg          100  F  SP LT\n",
       "   .8    11    14  COVINGTON              100  F  SP LT\n",
       "   .8    13    16  BEST VALUE             KING F  SP FF\n",
       "   .8    13    16  MONARCH                KING F  SP\n",
       "   .8    13    16  STERLING               KING F  SP\n",
       "   .9    10    13  SALEM                  KING F  SP LT\n",
       "   .9    11    10  SILVA THINS            100  F  HP FF\n",
       "   .9    11    11  AMERICAN               100  F  SP LT\n",
       "   .9    11    11  KOOL                   KING F  HP MILD\n",
       "   .9    11    11  MALIBU                 100  F  SP LT\n",
       "   .9    11    11  MONTCLAIR              KING F  SP LT\n",
       "   .9    11    11  PRIVATE STOCK          KING F  SP LT\n",
       "   .9    11    11  RIVIERA                100  F  SP LT SPECIALS\n",
       "   .9    11    11  RIVIERA                KING F  SP LT\n",
       "   .9    11    11  SALEM                  100  F  HP LT CUS-CASE\n",
       "   .9    11    12  KOOL                   100  F  SP MILD\n",
       "   .9    11    12  PRIME                  100  F  SP LT\n",
       "   .9    11    12  PRIVATE STOCK          100  F  SP LT\n",
       "   .9    11    12  PYRAMID                100  F  SP LT\n",
       "   .9    11    12  SUMMIT                 100  F  SP LT\n",
       "   .9    11    14  STYLE                  100  F  SP LT\n",
       "   .9    12    11  BENSON &amp; HEDGES        KING F  HP LT SPECIAL\n",
       "   .9    12    14  NEWPORT STRIPES        100  F  HP LT\n",
       "   .9    12    14  STYLE                  100  F  HP LT\n",
       "   .9    13    14  GPC                    100  F  SP FF\n",
       "   .9    13    15  MONARCH                KING F  HP\n",
       "   .9    13    16  DORAL                  100  F  SP FF\n",
       "   .9    14    13  SALEM                  100  F  HP SLIM\n",
       "   .9    14    14  COVINGTON              KING F  SP FF\n",
       "   .9    14    15  DORAL                  KING F  SP FF\n",
       "   .9    14    18  BEST VALUE             100  F  SP FF\n",
       "   .9    14    18  MONARCH                100  F  SP\n",
       "   .9    14    18  STERLING               100  F  SP\n",
       "  1.0    11    10  EVE 120                120  F  HP LT\n",
       "  1.0    11    11  MONTCLAIR              100  F  SP LT\n",
       "  1.0    11    16  MORE                   120  F  SP WHT-LT\n",
       "  1.0    12     7  CAPRI                  120  F  HP SUP-SLIM\n",
       "  1.0    12    11  MISTY SLIMS            120  F  HP LT SLIM\n",
       "  1.0    12    13  SATIN                  100  F  SP\n",
       "  1.0    12    18  MORE                   120  F  SP\n",
       "  1.0    12    NA  SPECIAL 10pkg          100  F  HP MILD\n",
       "  1.0    13    12  PLAYERS                100  F  HP\n",
       "  1.0    15    13  GPC                    KING F  SP FF\n",
       "  1.0    15    14  KOOL                   KING F  HP\n",
       "  1.0    15    14  KOOL                   KING F  HP\n",
       "  1.0    15    15  ALPINE                 KING F  HP FF\n",
       "  1.0    15    15  ALPINE                 KING F  SP\n",
       "  1.0    15    15  BASIC                  KING F  SP FF\n",
       "  1.0    15    15  PYRAMID                100  F  SP FF\n",
       "  1.0    16    15  MARLBORO               KING F  SP\n",
       "  1.0    16    15  RICHLAND 20'S          KING F  SP\n",
       "  1.1    13    13  KENT                   100  F  SP\n",
       "  1.1    14    12  VIRGINIA SLIMS         100  F  SP SLIM\n",
       "  1.1    14    13  MISTY SLIMS            100  F  HP FF SLIM\n",
       "  1.1    14    13  SARATOGA               120  F  HP\n",
       "  1.1    14    14  VIRGINIA SLIMS         120  F  HP LT SLIM\n",
       "  1.1    15    12  PRIVATE STOCK          KING F  SP FF\n",
       "  1.1    15    13  PRIME                  KING F  SP FF\n",
       "  1.1    15    14  ALPINE                 100  F  SP\n",
       "  1.1    15    14  PYRAMID                KING F  SP FF\n",
       "  1.1    15    14  SUMMIT                 100  F  SP FF\n",
       "  1.1    15    15  BENSON &amp; HEDGES        100  F  HP\n",
       "  1.1    15    15  BENSON &amp; HEDGES        100  F  SP\n",
       "  1.1    15    15  PM INTERNATIONAL       100  F  HP INT'L\n",
       "  1.1    15    15  PRIME                  100  F  SP FF\n",
       "  1.1    15    16  BASIC                  100  F  SP FF\n",
       "  1.1    15    16  PRIVATE STOCK          100  F  SP FF\n",
       "  1.1    16    14  MARLBORO               KING F  HP\n",
       "  1.1    16    15  KOOL                   KING F  SP\n",
       "  1.1    16    15  KOOL                   KING F  SP\n",
       "  1.1    16    16  RICHLAND 20'S          100  F  SP\n",
       "  1.2    15    16  MAX                    120  F  SP\n",
       "  1.2    16    13  RIVIERA                KING F  SP FF\n",
       "  1.2    16    13  SUMMIT                 KING F  SP FF\n",
       "  1.2    16    14  BENSON &amp; HEDGES        KING F  HP SPECIAL\n",
       "  1.2    16    14  MONTCLAIR              100  F  SP FF\n",
       "  1.2    16    16  KOOL                   100  F  SP\n",
       "  1.2    16    16  KOOL                   100  F  SP SUPER LONG\n",
       "  1.2    16    16  NEWPORT                KING F  HP\n",
       "  1.2    17    15  RIVIERA                KING F  HP\n",
       "  1.2    17    16  RIVIERA                100  F  SP\n",
       "  1.2    17    17  NEWPORT                KING F  SP\n",
       "  1.2    17    17  NEWPORT 10pkg          KING F  SP\n",
       "  1.2    17    17  NEWPORT 25pkg          KING F  SP\n",
       "  1.2    17    19  SALEM                  KING F  SP\n",
       "  1.3    18    17  RIVIERA                100  F  HP\n",
       "  1.3    21    13  KOOL                   REG  NF SP\n",
       "  1.4    17    19  SALEM                  100  F  SP\n",
       "  1.4    18    18  NEWPORT 10pkg          100  F  SP\n",
       "  1.4    18    19  NEWPORT                100  F  HP\n",
       "  1.4    18    19  NEWPORT                100  F  SP\n",
       "  1.4    19    18  NEWPORT 25pkg          100  F  SP\n",
       "  1.4    19    18  SALEM                  KING F  HP GOLD\n",
       "  1.5    17    17  TALL                   120  F  SP FF\n",
       "\n",
       "KEY\n",
       "---\n",
       "F   = Filter          NF = Non-Filter\n",
       "HP  = Hard Pack       SP = Soft Pack\n",
       "LT  = Light           FF = Full Flavor      DLX = Deluxe\n",
       "REG = Regular (70mm)  KS = King Size (80-85mm)\n",
       "100 and 120 are millimeters per cigarette.\n",
       "10, 25, or 30 = Number of cigarettes per pack.\n",
       "\n",
       "NIC = Nicotine in milligrams per cigarette.\n",
       "TAR = Total particulate matter in milligrams per cigarette less\n",
       "      nicotine and water.\n",
       "CO  = Carbon monoxide in milligrams per cigarette.\n",
       "\n",
       "Source: Report of Tar, Nicotine, and Carbon Monoxide of the Smoke of\n",
       "        1206 Varieties of Domestic Cigarettes For the Year 1994,\n",
       "        Federal Trade Commission; for more recent sources, see the\n",
       "        following note.\n",
       "</pre>\n",
       "<p>\n",
       "<b>NOTE:</b> As indicated above, the source for the numbers is the\n",
       "<a href=\"http://www.ftc.gov/os/1997/07/cigarett.htm\">\n",
       "Report of Tar, Nicotine, and Carbon Monoxide of the Smoke of 1206 Varieties of Domestic Cigarettes For the Year 1994</a>.\n",
       "Anyone using the tables should therefore check the numbers against the most recent available source.\n",
       "For FTC reports, this appears to be\n",
       "<a href=\"http://www.ftc.gov/reports/tobacco/1998tar&amp;nicotinereport.pdf\">\n",
       "Tar, Nicotine, and Carbon Monoxide of the Smoke of 1294 Varieties of Domestic Cigarettes For the Year 1998</a>.\n",
       "According to an article at\n",
       "<a href=\"http://freegovinfo.info/archive/200608\">http://freegovinfo.info/archive/200608</a>,\n",
       "the last report was released in September 1999 and the Federal Trade Commission has\n",
       "continued collecting data on nicotine but has not published reports on the findings.\n",
       "However, there is an FTC document posted in response to a request under the Freedom of Information Act at\n",
       "<a href=\"http://www.ftc.gov/foia/frequentrequests/foia2007-00482.pdf\">http://www.ftc.gov/foia/frequentrequests/foia2007-00482.pdf</a>.\n",
       "It appears to contain the tar, nicotine, and carbon monoxide content of most major brands of\n",
       "cigarettes from 1998 through 2005.  For more recent figures, see the section titled\n",
       "\"Searching Online for Recent Tar and Nicotine Numbers\" on <a href=\"smoke.html\">this page</a>.\n",
       "<p>\n",
       "Whichever source you use, be careful to note the precise brand and packaging as the content can\n",
       "vary widely in similar varieties.  For example, the figures given for Carlton cigarettes at\n",
       "<a href=\"http://www.bigsixsmokes.com/Carlton.shtml\">http://www.bigsixsmokes.com/Carlton.shtml</a>\n",
       "indicate that 120s have about five times the tar and nicotine content of Kings and 100s.\n",
       "<hr/>\n",
       "<a href=\"smoke.html\">\n",
       "Go to Smoking Home Page</a>\n",
       "<script type=\"text/javascript\">\n",
       "var gaJsHost = ((\"https:\" == document.location.protocol) ? \"https://ssl.\" : \"http://www.\");\n",
       "document.write(unescape(\"%3Cscript src='\" + gaJsHost + \"google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E\"));\n",
       "</script>\n",
       "<script type=\"text/javascript\">\n",
       "try {\n",
       "var pageTracker = _gat._getTracker(\"UA-15879323-1\");\n",
       "pageTracker._trackPageview();\n",
       "} catch(err) {}</script>\n",
       "<!-- Site Meter -->\n",
       "<script src=\"http://s45.sitemeter.com/js/counter.js?site=s45smoking\" type=\"text/javascript\">\n",
       "</script>\n",
       "<noscript>\n",
       "<a href=\"http://s45.sitemeter.com/stats.asp?site=s45smoking\" target=\"_top\">\n",
       "<img alt=\"Site Meter\" border=\"0\" src=\"http://s45.sitemeter.com/meter.asp?site=s45smoking\"/></a>\n",
       "</noscript>\n",
       "<!-- Copyright (c)2006 Site Meter --></p></p></body>\n",
       "</html>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_text = load_html(URL1,LOCAL1)\n",
    "soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b7839feb-f7bc-41fd-8d3d-6664c17d4098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 532 rows to Characteristics/ftc_1994_tnco_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, RequestException, Timeout, SSLError, ConnectionError\n",
    "from io import StringIO\n",
    "from typing import List, Union\n",
    "import os\n",
    "\n",
    "\n",
    "OUT_CSV  = \"Characteristics/ftc_1994_tnco_clean.csv\"\n",
    "# first url should be menthol cigs\n",
    "\n",
    "URL_MENTHOL = \"https://www.econdataus.com/cigms94.html\"\n",
    "URL_REGULAR = \"https://econdataus.com/cigra94.html\"\n",
    "\n",
    "\n",
    "\n",
    "# If you prefer to download first, set these to local filenames (e.g., 'cigms94.html')\n",
    "LOCAL1 = \"Characteristics/Nicotine, Tar, and CO Content of Menthol Cigarette Brands in 1994.htm\"  # e.g., \"cigms94.html\"\n",
    "LOCAL2 = \"Characteristics/Nicotine, Tar, and CO Content of Regular Cigarette Brands in 1994.htm\"  # e.g., \"cigra94.html\"\n",
    "\n",
    "\n",
    "TITLE_RE = re.compile(r'^\\s*NICOTINE,\\s*TAR,\\s*AND\\s*CO\\s*CONTENT\\s*OF\\s*DOMESTIC\\s*CIGARETTES\\s*IN\\s*1994\\s*$', re.I)\n",
    "SUB_MENTHOL_RE = re.compile(r'\\(.*Menthol\\s+Brands.*\\)', re.I)\n",
    "SUB_REGULAR_RE = re.compile(r'\\(.*Regular\\s+Brands.*\\)', re.I)\n",
    "HEADER_RE = re.compile(r'^\\s*NIC\\s+TAR\\s+CO\\s+BRAND\\s+NAME\\s+TYPE\\s*$', re.I)\n",
    "STOP_RE = re.compile(r'^\\s*KEY\\b', re.I)  # stop right before KEY section\n",
    "\n",
    "def load_html(url, local=None):\n",
    "    if local and os.path.exists(local):\n",
    "        with open(local, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "    r = requests.get(url, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def get_all_pre_texts(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    pres = [p.get_text(\"\\n\") for p in soup.find_all(\"pre\")]\n",
    "    if pres:\n",
    "        return pres\n",
    "    # fallback: regex if BS fails\n",
    "    return [m.group(1) for m in re.finditer(r\"<pre[^>]*>(.*?)</pre>\", html_text, re.S | re.I)]\n",
    "\n",
    "def slice_block(pre_text, want_menthol: bool):\n",
    "    \"\"\"\n",
    "    From a <pre> text:\n",
    "      - find TITLE line\n",
    "      - require the next non-blank line to include (Menthol Brands...) or (Regular Brands...)\n",
    "      - start from the table HEADER line (NIC TAR CO BRAND NAME TYPE)\n",
    "      - stop before a line that starts with KEY\n",
    "    Return the text slice including header; raise if not found.\n",
    "    \"\"\"\n",
    "    lines = pre_text.splitlines()\n",
    "    n = len(lines)\n",
    "\n",
    "    # find the title + correct subtitle block\n",
    "    start_idx = None\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if TITLE_RE.search(lines[i]):\n",
    "            # look ahead for the subtitle on subsequent non-empty line(s)\n",
    "            j = i + 1\n",
    "            while j < n and not lines[j].strip():\n",
    "                j += 1\n",
    "            if j < n:\n",
    "                ok = (SUB_MENTHOL_RE.search(lines[j]) if want_menthol else SUB_REGULAR_RE.search(lines[j]))\n",
    "                if ok:\n",
    "                    # from here, find the header line\n",
    "                    k = j + 1\n",
    "                    while k < n and not HEADER_RE.search(lines[k]):\n",
    "                        k += 1\n",
    "                    if k < n:\n",
    "                        # now collect until KEY\n",
    "                        out = []\n",
    "                        for t in range(k, n):\n",
    "                            if STOP_RE.search(lines[t]):\n",
    "                                break\n",
    "                            out.append(lines[t])\n",
    "                        if out:\n",
    "                            return \"\\n\".join(out)\n",
    "        i += 1\n",
    "\n",
    "    raise ValueError(\"Could not locate the data block with the expected title/subtitle/header.\")\n",
    "\n",
    "def parse_pre_to_df(block_text: str) -> pd.DataFrame:\n",
    "    # keep header so read_fwf learns column widths\n",
    "    return pd.read_fwf(StringIO(block_text))\n",
    "\n",
    "def get_col(df, key):\n",
    "    key = key.upper()\n",
    "    for c in df.columns:\n",
    "        cu = str(c).upper().strip()\n",
    "        if key == \"BRAND NAME\":\n",
    "            if \"BRAND\" in cu and \"NAME\" in cu:\n",
    "                return c\n",
    "        elif key in {\"NIC\",\"TAR\",\"CO\",\"TYPE\"}:\n",
    "            if cu == key or cu.startswith(key):\n",
    "                return c\n",
    "    raise KeyError(f\"Column {key} not found. Columns: {list(df.columns)}\")\n",
    "\n",
    "def build_clean_df(df_raw: pd.DataFrame, menthol_override=None) -> pd.DataFrame:\n",
    "    nic_col = get_col(df_raw, \"NIC\")\n",
    "    tar_col = get_col(df_raw, \"TAR\")\n",
    "    co_col  = get_col(df_raw, \"CO\")\n",
    "    brand_col = get_col(df_raw, \"BRAND NAME\")\n",
    "    type_col  = get_col(df_raw, \"TYPE\")\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df_raw.iterrows():\n",
    "        Tar = str(r[tar_col]).strip()\n",
    "        Nic = str(r[nic_col]).strip()\n",
    "        CO  = str(r[co_col]).strip()\n",
    "        Brand = str(r[brand_col]).strip()\n",
    "        Desc  = str(r[type_col]).strip()\n",
    "\n",
    "        size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC, DLX, SPECIAL, SUPSLIM, SLIM = parse_characteristics(Desc)\n",
    "        if menthol_override is not None:\n",
    "            MENTHOL = menthol_override\n",
    "\n",
    "        Tar_op, Tar_val = split_op_val(Tar)\n",
    "        Nic_op, Nic_val = split_op_val(Nic)\n",
    "        CO_op,  CO_val  = split_op_val(CO)\n",
    "        value_brand = 1 if Brand.upper() in VALUE_BRANDS else 0\n",
    "\n",
    "        rows.append([\n",
    "            Brand, Desc, size, F, NF, HP, SP, ULTRA, LT, FF, MENTHOL, GENERIC,\n",
    "            DLX, SPECIAL, SUPSLIM, SLIM,\n",
    "            Tar, Nic, CO, Tar_op, Tar_val, Nic_op, Nic_val, CO_op, CO_val,\n",
    "            0,  # manufacturer_tested (no * in HTML listing)\n",
    "            value_brand\n",
    "        ])\n",
    "\n",
    "    cols = [\n",
    "        \"Brand Name\",\"Description\",\"Size\",\"F\",\"NF\",\"HP\",\"SP\",\"Ultra_Lt\",\"Lt\",\"FF\",\"Menthol\",\n",
    "        \"Generic\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\n",
    "        \"Tar\",\"Nic\",\"CO\",\"Tar_op\",\"Tar_value\",\"Nic_op\",\"Nic_value\",\"CO_op\",\"CO_value\",\n",
    "        \"manufacturer_tested\",\"value_brand\"\n",
    "    ]\n",
    "    return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "\n",
    "def run(local1=None, local2=None, out_csv=OUT_CSV):\n",
    "    # Menthol page\n",
    "    html1 = load_html(URL_MENTHOL, LOCAL1)\n",
    "    pre_texts1 = get_all_pre_texts(html1)\n",
    "    block1 = None\n",
    "    for t in pre_texts1:\n",
    "        try:\n",
    "            block1 = slice_block(t, want_menthol=True)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    if block1 is None:\n",
    "        raise ValueError(\"Menthol block not found.\")\n",
    "\n",
    "    # Regular page\n",
    "    html2 = load_html(URL_REGULAR, LOCAL2)\n",
    "    pre_texts2 = get_all_pre_texts(html2)\n",
    "    block2 = None\n",
    "    for t in pre_texts2:\n",
    "        try:\n",
    "            block2 = slice_block(t, want_menthol=False)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    if block2 is None:\n",
    "        raise ValueError(\"Regular block not found.\")\n",
    "\n",
    "    df1_raw = parse_pre_to_df(block1)\n",
    "    df2_raw = parse_pre_to_df(block2)\n",
    "\n",
    "    df1 = build_clean_df(df1_raw, menthol_override=1)  # ALWAYS Menthol = 1 for first link\n",
    "    df2 = build_clean_df(df2_raw, menthol_override=0)\n",
    "\n",
    "    out = pd.concat([df1, df2], ignore_index=True)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"Wrote {len(out)} rows to {out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f3c5b21f-6281-4443-9857-fcb1b4b0e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "both = pd.concat([pd.read_csv(OUT_CSV),pd.read_csv(OUT)])\n",
    "both.to_csv( \"Characteristics/ftc_1994_1998_tnco_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68093d-82bb-406e-a916-9f170da3faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "both.col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e199a-590b-42f9-9642-73f195f65efc",
   "metadata": {},
   "source": [
    "# $\\text{UPC Cleaning}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9fd04ab1-7cef-4997-80b0-ba604084361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading UPC  : Dominick's\\upccig.csv\n",
      "Reading FTC  : Characteristics\\ftc_1994_1998_tnco_clean.csv\n",
      "Saving to    : Characteristics\n",
      "Wrote: Characteristics\\upccig_clean.csv\n",
      "Wrote: Characteristics\\upccig_with_tnco.csv\n",
      "Rows: 942 | matched to FTC at any level: 168\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv, re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# ===== 1) Set your PS2 root once =====\n",
    "ROOT = Path(r\"\")  # <- change if needed\n",
    "\n",
    "#ROOT = os.getcwd()\n",
    "\n",
    "UPC_PATH =  ROOT / \"Dominick's\" / \"upccig.csv\"\n",
    "FTC_PATH = ROOT / \"Characteristics\" / \"ftc_1994_1998_tnco_clean.csv\"\n",
    "\n",
    "OUT_DIR  = FTC_PATH.parent\n",
    "\n",
    "print(\"Reading UPC  :\", UPC_PATH)\n",
    "print(\"Reading FTC  :\", FTC_PATH)\n",
    "print(\"Saving to    :\", OUT_DIR)\n",
    "\n",
    "# ===== 2) Helpers =====\n",
    "ALIASES = {\n",
    "    r\"B & H\": \"BENSON & HEDGES\",\n",
    "    # r\"\\bMARL\\s*100S\\b\": \"MARLBORO 100\",   # example\n",
    "}\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = (s or \"\").upper()\n",
    "    for pat, repl in ALIASES.items():\n",
    "        s = re.sub(pat, repl, s)\n",
    "    s = re.sub(r\"[^\\w&]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "    return s\n",
    "\n",
    "def parse_size(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    if re.search(r\"(?<!\\d)100(?!\\d)|\\b100S?\\b\", t): return \"100\"\n",
    "    if re.search(r\"\\bKING(S)?\\b\", t):               return \"King\"\n",
    "    return \"Reg\"  # no missing\n",
    "\n",
    "def parse_pack(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    if re.search(r\"\\b(HP|HARD\\s*PACK|BOX|BOXES)\\b\", t): return \"HP\"\n",
    "    if re.search(r\"\\b(SP|SOFT\\s*PACK)\\b\", t):           return \"SP\"\n",
    "    return \"UNK\"  # fall back to brand+size\n",
    "\n",
    "def parse_char_dummies(text: str):\n",
    "    t = norm(text)\n",
    "    menthol = 1 if \"MENTHOL\" in t else 0\n",
    "    dlx     = 1 if re.search(r\"\\b(DLX|DELUXE)\\b\", t) else 0\n",
    "    special = 1 if re.search(r\"\\bSPECIAL\\b\", t) else 0\n",
    "    supslim = 1 if re.search(r\"\\b(SUPER\\s*SLIM|SUPSLIM|SUPERSLIM)\\b\", t) else 0\n",
    "    slim    = 1 if (not supslim) and re.search(r\"\\bSLIM(S)?\\b\", t) else 0\n",
    "    generic = 1 if re.search(r\"\\bGENERIC\\b\", t) else 0\n",
    "    carton  = 1 if re.search(r\"\\bCARTON(S)?\\b\", t) else 0\n",
    "    single  = 1 if re.search(r\"\\bSINGLE\\b\", t) else 0\n",
    "    pack_kw = 1 if re.search(r\"\\bPACK\\b|\\bPK\\b\", t) else 0\n",
    "    return menthol, dlx, special, supslim, slim, generic, carton, single, pack_kw\n",
    "\n",
    "def safe_float(x):\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s: return None\n",
    "    try: return float(s)\n",
    "    except:\n",
    "        try: return float(s.replace(\",\", \"\"))\n",
    "        except: return None\n",
    "\n",
    "# ---- brand lexicon from FTC (longest match, read with errors='replace') ----\n",
    "def build_brand_patterns(ftc_csv_path: Path):\n",
    "    brands = set()\n",
    "    with ftc_csv_path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            b = (row.get(\"Brand Name\") or \"\").strip()\n",
    "            if b: brands.add(b)\n",
    "    # a couple common value-brand aliases\n",
    "    brands |= {\"ALL AMERICAN VALUE\", \"AMERICAN VALUE\", \"ALL-AMERICAN VALUE\"}\n",
    "\n",
    "    pats = []\n",
    "    for b in brands:\n",
    "        tokens = [re.escape(x) for x in norm(b).split()]\n",
    "        if tokens:\n",
    "            pats.append((b, re.compile(r\"\\b\" + r\"\\s*\".join(tokens) + r\"\\b\")))\n",
    "    pats.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    return pats\n",
    "\n",
    "def find_brand(descrip: str, brand_patterns):\n",
    "    T = norm(descrip)\n",
    "    for brand, pat in brand_patterns:\n",
    "        if pat.search(T):\n",
    "            return brand\n",
    "    return \"\"\n",
    "\n",
    "# ---- FTC aggregates by (brand,size,pack) → (brand,size) → (brand) ----\n",
    "def load_ftc_aggregates(ftc_csv_path: Path):\n",
    "    b_sp = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "    b_s  = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "    b    = defaultdict(lambda: {\"tar\": [], \"nic\": [], \"co\": []})\n",
    "\n",
    "    with ftc_csv_path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        has_val = {\"Tar_value\",\"Nic_value\",\"CO_value\"}.issubset(r.fieldnames or [])\n",
    "        for row in r:\n",
    "            brand = (row.get(\"Brand Name\") or \"\").strip()\n",
    "            if not brand: \n",
    "                continue\n",
    "            size  = (row.get(\"Size\") or \"Reg\").strip()\n",
    "\n",
    "            hp = str(row.get(\"HP\") or \"\").strip()\n",
    "            sp = str(row.get(\"SP\") or \"\").strip()\n",
    "            pack = \"HP\" if hp in (\"1\",\"True\",\"TRUE\",\"true\") else (\"SP\" if sp in (\"1\",\"True\",\"TRUE\",\"true\") else \"UNK\")\n",
    "\n",
    "            if has_val:\n",
    "                tar = safe_float(row.get(\"Tar_value\"))\n",
    "                nic = safe_float(row.get(\"Nic_value\"))\n",
    "                co  = safe_float(row.get(\"CO_value\"))\n",
    "            else:\n",
    "                def num_from_raw(v):\n",
    "                    s = (v or \"\").strip()\n",
    "                    s = s[1:] if s.startswith(\"<\") else s\n",
    "                    return safe_float(s)\n",
    "                tar = num_from_raw(row.get(\"Tar\"))\n",
    "                nic = num_from_raw(row.get(\"Nic\"))\n",
    "                co  = num_from_raw(row.get(\"CO\"))\n",
    "\n",
    "            for name, val in ((\"tar\",tar), (\"nic\",nic), (\"co\",co)):\n",
    "                if val is None: \n",
    "                    continue\n",
    "                b_sp[(brand,size,pack)][name].append(val)\n",
    "                b_s[(brand,size)][name].append(val)\n",
    "                b[(brand,)][name].append(val)\n",
    "\n",
    "    def reduce_means(B):\n",
    "        out = {}\n",
    "        for k, vv in B.items():\n",
    "            def m(x): return sum(x)/len(x) if x else None\n",
    "            out[k] = (m(vv[\"tar\"]), m(vv[\"nic\"]), m(vv[\"co\"]))\n",
    "        return out\n",
    "\n",
    "    return reduce_means(b_sp), reduce_means(b_s), reduce_means(b)\n",
    "\n",
    "# ===== 3) Clean & merge (reads UPC with errors='replace') =====\n",
    "def clean_and_merge(UPC_PATH: Path, FTC_PATH: Path, OUT_DIR: Path):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_clean = OUT_DIR / \"upccig_clean.csv\"\n",
    "    out_merge = OUT_DIR / \"upccig_with_tnco.csv\"\n",
    "\n",
    "    brand_patterns = build_brand_patterns(FTC_PATH)\n",
    "    by_bsp, by_bs, by_b = load_ftc_aggregates(FTC_PATH)\n",
    "\n",
    "    with UPC_PATH.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as fin, \\\n",
    "         out_clean.open(\"w\", newline=\"\", encoding=\"utf-8\") as fclean, \\\n",
    "         out_merge.open(\"w\", newline=\"\", encoding=\"utf-8\") as fmerge:\n",
    "\n",
    "        rin = csv.DictReader(fin)\n",
    "        cols_in = rin.fieldnames or []\n",
    "        clean_cols = cols_in + [\"brand\",\"size\",\"pack\",\"Menthol\",\"Dlx\",\"Special\",\"SupSlim\",\"Slim\",\"Generic\",\"Carton\",\"Single\",\"Pack_kw\"]\n",
    "        rout_clean = csv.DictWriter(fclean, fieldnames=clean_cols)\n",
    "        rout_clean.writeheader()\n",
    "\n",
    "        merge_cols = clean_cols + [\"tnco_key_level\",\"Tar_mean\",\"Nic_mean\",\"CO_mean\"]\n",
    "        rout_merge = csv.DictWriter(fmerge, fieldnames=merge_cols)\n",
    "        rout_merge.writeheader()\n",
    "\n",
    "        n, matched = 0, 0\n",
    "        for row in rin:\n",
    "            n += 1\n",
    "            descr = (row.get(\"DESCRIP\") or row.get(\"DESCR\") or \"\").strip()\n",
    "\n",
    "            brand = find_brand(descr, brand_patterns) or \"\"   # NA if not found\n",
    "            size  = parse_size(descr)\n",
    "            pack  = parse_pack(descr)                         # HP / SP / UNK\n",
    "\n",
    "\n",
    "            ######### HARDCODED bc why not\n",
    "            if descr == 'NAME-GENERIC CIGS KI':\n",
    "                size = \"King\"\n",
    "\n",
    "            Menthol, Dlx, Special, SupSlim, Slim, Generic, Carton, Single, Pack_kw = parse_char_dummies(descr)\n",
    "\n",
    "            clean_row = dict(row)\n",
    "            clean_row.update({\n",
    "                \"brand\": brand if brand else \"NA\",\n",
    "                \"size\": size,\n",
    "                \"pack\": pack,\n",
    "                \"Menthol\": Menthol, \"Dlx\": Dlx, \"Special\": Special,\n",
    "                \"SupSlim\": SupSlim, \"Slim\": Slim, \"Generic\": Generic,\n",
    "                \"Carton\": Carton, \"Single\": Single, \"Pack_kw\": Pack_kw\n",
    "            })\n",
    "            rout_clean.writerow(clean_row)\n",
    "\n",
    "            # Merge priority: (brand,size,pack) → (brand,size) → (brand)\n",
    "            tnco_key_level = \"none\"\n",
    "            Tar_mean = Nic_mean = CO_mean = \"\"\n",
    "\n",
    "            if brand:\n",
    "                if pack in (\"HP\",\"SP\") and (brand,size,pack) in by_bsp:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_bsp[(brand,size,pack)]; tnco_key_level = \"brand+size+pack\"\n",
    "                elif (brand,size) in by_bs:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_bs[(brand,size)]; tnco_key_level = \"brand+size\"\n",
    "                elif (brand,) in by_b:\n",
    "                    Tar_mean, Nic_mean, CO_mean = by_b[(brand,)]; tnco_key_level = \"brand\"\n",
    "\n",
    "            matched += (tnco_key_level != \"none\")\n",
    "\n",
    "            merge_row = dict(clean_row)\n",
    "            merge_row.update({\n",
    "                \"tnco_key_level\": tnco_key_level,\n",
    "                \"Tar_mean\": f\"{Tar_mean:.3f}\" if isinstance(Tar_mean,float) else \"\",\n",
    "                \"Nic_mean\": f\"{Nic_mean:.3f}\" if isinstance(Nic_mean,float) else \"\",\n",
    "                \"CO_mean\":  f\"{CO_mean:.3f}\"  if isinstance(CO_mean, float) else \"\",\n",
    "            })\n",
    "            rout_merge.writerow(merge_row)\n",
    "\n",
    "    print(f\"Wrote: {out_clean}\")\n",
    "    print(f\"Wrote: {out_merge}\")\n",
    "    print(f\"Rows: {n} | matched to FTC at any level: {matched}\")\n",
    "\n",
    "# run\n",
    "clean_and_merge(UPC_PATH, FTC_PATH, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2ddef451-1fea-4117-b4af-b2b70bde32e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: F:\\Codes\\Jupyter\\ECON847\\PS2\\Characteristics\\upccig_with_tnco.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\Codes\\\\Jupyter\\\\ECON847\\\\PS2\\\\Characteristics\\\\upccig_with_tnco.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-005e17775892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mupc_has_brand\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mINPATH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[1;32m-> 1186\u001b[1;33m                        opener=self._opener)\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\Codes\\\\Jupyter\\\\ECON847\\\\PS2\\\\Characteristics\\\\upccig_with_tnco.csv'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv, re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---- set your PS2 root folder once ----\n",
    "ROOT = Path(r\"F:\\Codes\\Jupyter\\ECON847\\PS2\")   # <-- adjust if needed\n",
    "INPATH = ROOT / \"Characteristics\" / \"upccig_with_tnco.csv\"\n",
    "OUTDIR = INPATH.parent\n",
    "\n",
    "print(\"Reading:\", INPATH)\n",
    "\n",
    "def is_clear_brand(b):\n",
    "    b = (b or \"\").strip().upper()\n",
    "    return b not in {\"\", \"NA\", \"N/A\", \"UNK\", \"UNKNOWN\"}\n",
    "\n",
    "# find a UPC column name robustly\n",
    "def pick_upc_col(fieldnames):\n",
    "    if not fieldnames: return None\n",
    "    # prefer exact names, then any column containing 'UPC'\n",
    "    prefs = [\"UPC\", \"UPC12\", \"UPC_CODE\", \"UPCNUM\", \"UPC CODE\", \"UPCNUMBER\"]\n",
    "    for p in prefs:\n",
    "        if p in fieldnames: return p\n",
    "    for fn in fieldnames:\n",
    "        if re.search(r\"\\bUPC\\b\", fn, re.I): return fn\n",
    "    return None\n",
    "\n",
    "n_rows = 0\n",
    "uniq_descr = set()\n",
    "uniq_upc   = set()\n",
    "rows_with_brand = 0\n",
    "\n",
    "# coverage by description / by upc\n",
    "descr_has_brand = defaultdict(lambda: False)\n",
    "upc_has_brand   = defaultdict(lambda: False)\n",
    "\n",
    "with INPATH.open(\"r\", encoding=\"utf-8\", errors=\"replace\", newline=\"\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    fields = r.fieldnames or []\n",
    "    upc_col = pick_upc_col(fields)\n",
    "    if upc_col is None:\n",
    "        print(\"WARNING: Couldn't find a UPC column; I'll treat all UPCs as blank.\")\n",
    "\n",
    "    for row in r:\n",
    "        n_rows += 1\n",
    "        descr = (row.get(\"DESCRIP\") or row.get(\"DESCR\") or \"\").strip()\n",
    "        upc   = (row.get(upc_col) if upc_col else \"\").strip()\n",
    "        brand = row.get(\"brand\", \"\")\n",
    "\n",
    "        uniq_descr.add(descr)\n",
    "        uniq_upc.add(upc)\n",
    "\n",
    "        if is_clear_brand(brand):\n",
    "            rows_with_brand += 1\n",
    "            descr_has_brand[descr] = True\n",
    "            upc_has_brand[upc]     = True\n",
    "\n",
    "# compute ratios\n",
    "row_ratio     = rows_with_brand / n_rows if n_rows else 0.0\n",
    "descr_ratio   = (sum(descr_has_brand.values()) / len(uniq_descr)) if uniq_descr else 0.0\n",
    "upc_ratio     = (sum(upc_has_brand.values())   / len(uniq_upc))   if uniq_upc else 0.0\n",
    "\n",
    "print(f\"Rows total: {n_rows:,}\")\n",
    "print(f\"Unique DESCRIP: {len(uniq_descr):,}\")\n",
    "print(f\"Unique UPC: {len(uniq_upc):,}\")\n",
    "print(f\"Rows with clear brand: {rows_with_brand:,}  ({row_ratio:.2%})\")\n",
    "print(f\"DESCRIP coverage (unique descriptions with a clear brand): {descr_ratio:.2%}\")\n",
    "print(f\"UPC coverage (unique UPCs with a clear brand): {upc_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28a428-8ebe-48f8-a5fa-117c817074e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
